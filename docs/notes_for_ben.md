# Notes for Ben

## 2025-12-18: Fixed Accuracy Scoring to Use Negative Absolute Error

**What was changed**:
- Modified accuracy calculation in `src/r/clean_data.R` to use **negative absolute error** instead of absolute error
- This means **higher scores now indicate better accuracy**, making interpretation more intuitive
- Removed the `abs()` wrapper that was flipping signs for reporting (line 326 in report.qmd)
- Updated all plot axis labels and table notes to reflect the new scoring direction

**Specific changes**:
1. **clean_data.R:194-196**: Changed from `abs(green_outgroup_pre/post - actual_green)` to `-abs(...)`
2. **report.qmd:326**: Removed `abs()` wrapper around `accuracy_time_coef`
3. **report.qmd**: Updated all axis labels from "Absolute Error" to "(higher = more accurate)"
4. **report.qmd**: Updated table notes to say "negative absolute error (higher = more accurate)"

**Why this is better**:
- Aligns accuracy with other measures (warmth, confidence) where higher = better
- Eliminates confusion about direction of effects
- Removes need for sign-flipping workarounds that were specific to Dunning-Kruger analyses
- Makes correlation interpretations straightforward (positive correlation = more X, more accuracy)

**Action needed**:
- Re-run `src/r/clean_data.R` to regenerate the cleaned data with new accuracy scores
- Then re-run `src/r/create_analysis_data.R` and render the report
- Or simply run `make all` to regenerate the entire pipeline

---

## 2025-12-17: Writing Review of report.qmd

I've reviewed your report and identified areas for improvement based on your writing guidelines. Here are the main issues organized by priority:

### 1. PASSIVE VOICE → ACTIVE VOICE

**Line 53-54** (Data Collection)
- Current: "Data were collected via Qualtrics from `r nrow(dat)` participants."
- Suggest: "We collected data via Qualtrics from `r nrow(dat)` participants."

**Line 119** (Preliminaries)
- Current: "Democrats' and Republicans' actual green values and their perceptions of ingroup and outgroup attitudes are shown in the figure below."
- Suggest: "The figure below shows Democrats' and Republicans' actual green values and their perceptions of ingroup and outgroup attitudes."
- OR BETTER: "Figure X compares actual green values with perceived ingroup and outgroup attitudes for Democrats and Republicans."

**Line 289-290** (Q2)
- Current: "While both groups become more accurate, the effect is driven by democrats becoming less biased about republicans"
- Suggest: "While both groups become more accurate, Democrats drive this effect by becoming substantially less biased about Republicans"

**Line 351-352** (Q3)
- Current: "More politically extreme participants were more likely to be biased about the outgroup (TODO ADD STATS) and feel less warm about them"
- Suggest: "Politically extreme participants showed greater bias about the outgroup (TODO ADD STATS) and felt less warm toward them"

**Line 865** (Limitations)
- Current: "Several limitations should be noted."
- Suggest: "This study has several limitations." OR "We note several limitations."

### 2. WEAK TOPIC SENTENCES

**Line 103** (Preliminaries)
- Current: "Where does our sample fall in the political spectrum?"
- Suggest: "Our sample consists of highly partisan participants." OR "Most participants held strong partisan identities, with the majority in the top and bottom quartiles of the political spectrum."

**Lines 66-67** (Measures section)
- Current: Just a header "**Primary Outcomes**" with no intro
- Suggest: Add a topic sentence before diving into the list: "We measured two primary outcomes to assess belief updating and affective change."

**Line 855** (Discussion - Summary of Findings)
- Current: "TODO: read what i wrote and write a straight forward summary."
- This needs to be written, but when you do, start with a clear topic sentence like: "This study demonstrates that brief AI-mediated interactions can reduce political misperceptions, though effects vary by partisan group."

### 3. WORDINESS & UNCLEAR LANGUAGE

**Line 41-42** (Overview)
- Current: Three-sentence opener is wordy and buries the lede
- Suggest combining and sharpening: "Political polarization creates marketing challenges when firms must appeal to ideologically diverse consumers. Partisans hold exaggerated and inaccurate views of political outgroups, leading to ineffective marketing strategies. This study tests whether interacting with an AI chatbot representing a political outgroup can improve marketers' understanding and reduce affective polarization."

**Line 103-104** (Preliminaries)
- Current: "Where does our sample fall in the political spectrum? As shown in the figure below, most participants were partisan, with most lying in the top and bottom 25%."
- Issues: (1) Repetitive "most", (2) "Where does our sample fall" is conversational not direct
- Suggest: "Most participants held strongly partisan views, falling in the top and bottom quartiles of the political spectrum."

**Line 520-521** (Q5)
- Current: "The pre-registration specified examining whether engagement with the chatbot—operationalized as the number of words typed by participants during the chat—is associated with post-interaction outcomes."
- Too wordy; suggest: "We measured engagement as the number of words participants typed during the chat and tested whether greater engagement predicted larger changes in beliefs and warmth."

**Line 677-678** (Q7)
- Current: "As shown in the figure below, republican participants learning about democrats become more confident after talking to the bot (despite actually learning less than democrats). Democrats do not become more confident."
- Issues: (1) "As shown in the figure below" is overused throughout, (2) Could be more concise
- Suggest: "Republicans learning about Democrats gained confidence after the interaction, despite learning less than Democrats. Democrats' confidence did not change."

### 4. PARALLEL STRUCTURE ISSUES

**Lines 45-48** (Hypotheses)
Current list structure is inconsistent:
1. "**Report more accurate beliefs** about the outgroup's attitudes..."
2. "**Feel warmer** toward the political outgroup..."

Suggest making them more parallel:
1. "**Report more accurate beliefs** about the outgroup's attitudes toward environmentally responsible consumption (Primary Outcome 1)."
2. "**Report warmer feelings** toward the political outgroup (Primary Outcome 2)."

**Lines 1139-1152** (Extreme cases patterns)
Current lists are GOOD - keep these as a model:
- Both start with past tense verbs
- Both have parallel bullet structure
- This is exactly what parallel structure should look like

### 5. INCONSISTENT TENSE

**Line 289-292** (Q2)
- Current: Mixes present ("become") with past in other sections
- Throughout Results, standardize to past tense: "became more accurate" not "become more accurate"

### 6. REPETITIVE PHRASES

**Overused: "As shown in the figure below"**
- Appears at lines: 103, 119, 677, and probably others
- Vary your transitions: "Figure X reveals...", "The data show...", or just describe directly: "Republicans learning about Democrats gained confidence..."

**Overused: "TODO ADD STATS"**
- Lines 290, 292, 351, 352, 453
- Either add the stats or remove these placeholders before calling it done

### 7. SPECIFIC SMALLER FIXES

**Line 224**: "more warm" → "warmer"

**Line 119**: "The real gap at baseline is much smaller than the *perceived* gap."
- Good directness, but "real" vs "perceived" could be clearer: "The actual gap at baseline was much smaller than participants perceived."

**Line 859-861** (Implications)
- Current: "If the hypothesized effects are supported, this research suggests..."
- Issue: You already HAVE the results, so don't hedge with "if"
- Suggest: "These findings suggest that AI-mediated interactions can serve as a scalable tool..."

### OVERALL PATTERNS TO WATCH:

1. **Passive voice creep**: Search for "were", "was", "is shown", "should be noted" and convert to active
2. **Weak verbs**: "can serve as", "may be", "suggests that" - be more direct when you have evidence
3. **Conversational questions as topic sentences**: Replace with declarative statements
4. **Over-hedging in Discussion**: You have results - own them (while noting limitations separately)

### STRENGTHS TO MAINTAIN:

- Your extreme cases analysis (lines 1037-1154) has excellent writing: strong topic sentences, parallel structure in lists, active voice
- Q1 and Q4 openings are direct and clear
- Your use of inline statistics is appropriate and well-integrated
- The hypotheses section is clear and testable

---

Priority order for revisions:
1. Fix all passive voice (15 min)
2. Strengthen topic sentences for each section (20 min)
3. Remove "TODO" placeholders or fill them in (10 min)
4. Make tense consistent throughout Results (10 min)
5. Vary transition phrases (10 min)

Total estimated revision time: ~65 minutes for major improvements

---

## 2025-12-17: Simple Effects Added to All MLM Tables

**What was done**:
Following the pattern from Q2, added simple effects/slopes at the top of all multilevel model tables in [report.qmd](../analysis/report.qmd)

**Changes made**:

**Q2 (Time × Party interaction):**
- Already had simple effects of time for each learner party
- Shows whether D→R and R→D participants updated differently

**Q3 (Extremism moderation) - UPDATED:**
- Shows time effects (pre-to-post change) at Low Extremism (-1 SD) and High Extremism (+1 SD)
- Uses `emmeans()` to compute effects at specific extremism levels
- Directly answers whether people at different extremism levels update differently
- Table grouped: "Simple Effects by Extremism Level" at top, "Full Regression Results" below

**Q7 (Confidence changes) - NEW:**
- Added simple effects of time for each learner party
- Uses `emmeans()` with contrast to show pre-to-post change for D→R and R→D
- Table grouped: "Simple Effects by Political Affiliation" at top, "Full Regression Results" below

**Formatting approach**:
- All tables use `add_rows` with position attributes to insert simple effects at top
- Use `group_tt()` to separate simple effects section from full regression results
- Use `style_tt()` for bold headers and horizontal separator lines
- Consistent presentation makes it easy to directly answer each research question

**Why this helps**:
Makes tables more reader-friendly by highlighting the key effects that directly answer each research question before showing the full model output with all coefficients.

## 2025-12-17: Makefile Added for Reproducible Pipeline

**What was done**:
- Created [Makefile](Makefile) to automate the entire analysis pipeline
- Updated [README.md](README.md) with comprehensive documentation

**Makefile features**:
- **`make all`** - Runs entire pipeline from raw data to final reports
- **`make help`** - Shows all available commands
- **`make data`** - Processes data and downloads conversations
- **`make analysis`** - Creates analysis datasets
- **`make report`** - Renders HTML and PDF reports
- **`make clean`** - Removes all processed data/outputs
- **`make dev`** - Quick HTML-only render for iteration

**Pipeline steps**:
1. Clean raw Qualtrics data (`src/r/clean_data.R`)
2. Download conversations (`src/python/download_conversations.py`)
3. Create analysis dataset (`src/r/create_analysis_data.R`)
4. Analyze extreme cases (`src/r/analyze_extreme_cases.R`)
5. Render reports (`analysis/report.qmd`)

**Benefits**:
- Single command reproducibility (`make all`)
- Dependency tracking (only rebuilds what's needed)
- Clear documentation of pipeline steps
- Easy for collaborators and reviewers

---

## 2025-12-17: Extreme Cases Appendix Added

**What was done**:
- Created [src/r/analyze_extreme_cases.R](src/r/analyze_extreme_cases.R) to identify and analyze extreme cases
- Added comprehensive **Appendix: Extreme Cases Analysis** to [analysis/report.qmd](analysis/report.qmd:753-872)
- Generated [output/extreme_cases_summary.md](output/extreme_cases_summary.md) with detailed findings

**Appendix includes**:
1. Summary table comparing most vs. least improved participants by learner party
2. Full marketing messages from extreme cases with interpretation
3. Engagement comparison table (word count, turn count)
4. Qualitative patterns distinguishing successful vs. unsuccessful learners
5. **Complete conversation transcripts** for all 4 extreme cases with detailed analysis

**Key insights**:
- Most improved: Genuine curiosity + deep engagement + finding common ground → bridge-building messages
- Least improved (worsened): Hostility + confirmation bias + surface engagement → divisive messages
- Full conversation transcripts available in data/processed/conversations/ for deeper analysis

---

## 2025-12-17: File Organization and Data Structure Completed

**Files reorganized** according to best practices:

**Source code organized** by language:
- R scripts → `src/r/` (clean_data.R, create_analysis_data.R, etc.)
- Python scripts → `src/python/` (download_conversations.py)

**Data files** (moved to `data/processed/`):
- `cleaned_data.parquet` - Full cleaned dataset with all variables
- `conversation_metrics.parquet` - Conversation engagement metrics
- `conversations/*.json` - Individual conversation files (211 participants)
- `analysis_data.parquet` - **Minimal dataset for publication** (50 variables)
- `analysis_data.csv` - Same as above, CSV format for broader accessibility

**Images** (moved to `output/figures/`):
- `accuracy_change.png`
- `warmth_change.png`

**Scripts updated** with new paths:
- [src/r/clean_data.R](src/r/clean_data.R:200-214) - Now writes to `data/processed/`
- [src/python/download_conversations.py](src/python/download_conversations.py:81-120) - Now writes to `data/processed/`
- [analysis/report.qmd](analysis/report.qmd:24) - Now reads from `data/processed/`
- [src/r/create_analysis_data.R](src/r/create_analysis_data.R) - **NEW** script to create minimal publication dataset

**Tested**: All scripts run successfully with new paths. Analysis reproduces correctly.

**Summary**:
- Full dataset: 141 variables, 211 observations
- **Publication dataset**: 50 variables, 211 observations (available as `.parquet` and `.csv`)
- All paths updated and verified working
- Documentation added to [data/processed/README.md](data/processed/README.md)
- Project instructions updated in [CLAUDE.md](CLAUDE.md)

---

## 2025-12-17: Q5 Engagement Analysis Completed

**What was done**:
1. Downloaded all conversations from the database for 211 participants
2. Saved individual conversations as JSON files in `output/conversations/`
3. Computed conversation metrics:
   - `user_turn_count`: Number of messages sent by participant (mean = 10.06, range = 1-37)
   - `user_word_count`: Total words typed by participant (mean = 182.85, range = 47-734)
   - `has_conversation`: Boolean indicating conversation data available (all TRUE)
4. Merged metrics into cleaned data via [src/clean_data.R](src/clean_data.R:199-208)
5. Implemented Q5 engagement analysis in [analysis/report.qmd](analysis/report.qmd:290-387)

**Q5 Analysis Completed**:
- Tested whether engagement predicts accuracy change using:
  - Turn count (number of messages)
  - Log-transformed word count (no +1 needed; min word count = 47)
- Tested whether engagement predicts warmth change (both metrics)
- Created 2×2 visualization grid showing all engagement effects by learner party
- Models control for baseline accuracy/warmth and learner party
- Analysis follows pre-registration specification

**Files created**:
- [src/python/download_conversations.py](src/python/download_conversations.py) - Script to download conversations
- `output/conversations/*.json` - 211 individual conversation files
- `output/conversation_metrics.parquet` - Metrics for all participants

---

## 2025-12-17: Fixed Line Plot Colors

Fixed color mapping issue in all line plots. The problem was that `learner_party` has values "D→R" and "R→D", but plots were using `party_colors` defined for "Democrat" and "Republican".

**Solution**: Created new `learner_party_colors` mapping:
- D→R (Democrats learning about Republicans) = Blue (#0015BC)
- R→D (Republicans learning about Democrats) = Red (#E81B23)

**Plots updated**:
- Belief Accuracy plot (analysis/report.qmd:161)
- Outgroup Warmth plot (analysis/report.qmd:171)
- Empathy vs. Informativeness scatter plot (analysis/report.qmd:280)
- Confidence plot (analysis/report.qmd:337)

---

## Data Needed for Complete Analysis

### Q5: Engagement Analysis
- **STATUS: ✓ DATA NOW AVAILABLE**
- **Variables added**: `user_turn_count` and `user_word_count` in cleaned data
- **Purpose**: Test whether engagement predicts belief updating and warmth change
- **Pre-registration**: "examine whether engagement with the chatbot—operationalized as the number of words typed by the participant during the chat—is associated with post-interaction outcomes"

### Q6: Applied Judgment (Slogan Quality)
- **Available**: Raw slogan text in variable `Q23`
- **Missing**:
  1. Semantic similarity scores (comparing to AI-generated responses from ChatGPT, Claude, Gemini)
  2. Quality ratings (human or LLM ratings of fit to target group)
- **Pre-registration**: "assess potential copying by computing semantic similarity... Slogans with similarity values more than 3 SDs above the mean will be excluded"
- **Action needed**:
  1. Generate comparison slogans from ChatGPT/Claude/Gemini
  2. Compute semantic embeddings and similarity scores
  3. Get quality ratings for slogans

## Report Organization Complete

The report has been reorganized around 7 research questions:
1. Q1: Do people update their beliefs? (accuracy + warmth)
2. Q2: Is updating symmetric across groups? (time × party interaction)
3. Q3: Does extremity predict updating? (extremism moderator)
4. Q4: Is updating linked to bot perceptions? (info/empathy regressions)
5. Q5: Does engagement matter? (PLACEHOLDER - need word count data)
6. Q6: Does it improve applied judgment? (PLACEHOLDER - need slogan ratings)
7. Q7: Does confidence track accuracy? (confidence analysis)

Robustness checks (warmth difference score) are at the end of Results section.

## Code Organization Complete

All data preparation has been moved to src/clean_data.R:
- Accuracy variables (accuracy_pre, accuracy_post) are now computed in the cleaning script
- Computed as absolute error: abs(green_outgroup_pre/post - actual_green)
- Actual outgroup attitudes calculated from within-group means
- report.qmd now uses pre-computed accuracy variables from cleaned data

## Makefile Pipeline Fixed (Dec 17, 2024)

Fixed the Makefile to correctly run the full pipeline starting from `src/r/download_qualtrics.r`:

**Pipeline sequence:**
1. **Step 0**: Download raw Qualtrics data (`src/r/download_qualtrics.r` → `data/raw/qualtrics.parquet`)
2. **Step 1**: Initial data cleaning without conversations (`src/r/clean_data.R`)
3. **Step 2**: Download conversations from database (`src/python/download_conversations.py` → `data/processed/conversation_metrics.parquet`)
4. **Step 3**: Re-clean data with conversation metrics merged (`src/r/clean_data.R`)
5. **Step 4-6**: Run analyses (create analysis dataset, extreme cases)
6. **Step 7-8**: Render HTML and PDF reports

**Key fixes:**
- Added virtual environment activation: `source .venv/bin/activate && python`
- Fixed dependency chain so conversation metrics are merged into cleaned data
- Reports now depend on both `cleaned_data.parquet` and `conversation_metrics.parquet`
- Pipeline tested and working with `make all`

**All 306 participants' conversations downloaded successfully:**
- Mean user turns: 10.6 (range: 1-93)
- Mean user words: 189.2 (range: 47-734)