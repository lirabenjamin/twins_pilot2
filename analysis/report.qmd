---
title: "Learning from Twins: Can marketers learn about consumers across the political divide by interacting with AI"
author: "Benjamin Lira"
date: today
bibliography: ../lit/references.bib
csl: https://www.zotero.org/styles/apa
link-citations: true
format:
  html:
    default: true
    fig-cap-location: bottom
    tbl-cap-location: top
  pdf:
    keep-tex: true
    fig-cap-location: bottom
    tbl-cap-location: top
execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
library(tidyverse)
library(arrow)
library(lme4)
library(lmerTest)
library(modelsummary)
library(gt)
library(ggplot2)
library(ggpubr)
library(patchwork)
library(emmeans)
library(tinytable)
library(jsonlite)
library(effectsize)

# switch for data load.
is_running_in_quarto <- function() {
  # Check if a Quarto-specific environment variable is set
  return(Sys.getenv("QUARTO_DOCUMENT_PATH") != "")
}

# Example usage:
if (is_running_in_quarto()) {
  dat <- read_parquet("../data/processed/analysis_data.parquet")
  fig_path <- "../output/figures/"
  tab_path <- "../output/tables/"
} else {
 dat <- read_parquet("./data/processed/analysis_data.parquet")
 fig_path <- "./output/figures/"
 tab_path <- "./output/tables/"
}

# Create output directories if they don't exist
dir.create(fig_path, recursive = TRUE, showWarnings = FALSE)
dir.create(tab_path, recursive = TRUE, showWarnings = FALSE)

# Helper function to save tables
save_table <- function(table_obj, filename, path = tab_path) {
  full_path <- paste0(path, filename)

  # Check if it's a gt table
  if ("gt_tbl" %in% class(table_obj)) {
    gtsave(table_obj, paste0(full_path, ".html"))
    gtsave(table_obj, paste0(full_path, ".png"))
  }
  # Check if it's a tinytable (from modelsummary)
  else if ("tinytable" %in% class(table_obj)) {
    save_tt(table_obj, paste0(full_path, ".html"), overwrite = TRUE)
    save_tt(table_obj, paste0(full_path, ".png"), overwrite = TRUE)
  }

  return(table_obj)
}

# Load cleaned data


# Define consistent color scheme: Democrats = blue, Republicans = red
party_colors <- c("Democrat" = "#0015BC", "Republican" = "#E81B23")

# Define learner party colors (D→R = Democrat learning about Republicans, R→D = Republican learning about Democrats)
learner_party_colors <- c("D→R" = "#0015BC", "R→D" = "#E81B23")

# Define common coefficient mapping for all modelsummary tables
coef_map_common <- c(
  "timepost" = "Time (Post)",
  "learner_partyR→D" = "Learner Party (R→D)",
  "political_extremism" = "Political Extremism",
  "timepost:learner_partyR→D" = "Time × Learner Party",
  "timepost:political_extremism" = "Time × Extremism",
  "learner_partyR→D:political_extremism" = "Learner Party × Extremism",
  "timepost:learner_partyR→D:political_extremism" = "Time × Learner Party × Extremism",
  "accuracy_pre" = "Baseline Accuracy",
  "warmth_outgroup_pre" = "Baseline Warmth",
  "confidence_outgroup_pre" = "Baseline Confidence",
  "info" = "Bot Informativeness",
  "empathy" = "Bot Empathy",
  "user_turn_count" = "Turn Count",
  "log_word_count" = "Log(Word Count)",
  "SD (Intercept ResponseID)" = "SD(Intercept)"
)
```

## Overview

Political polarization  @example2024 creates challenges for marketers who must communicate effectively with ideologically [@example2024] diverse consumers. Misperceptions about political outgroups are widespread, with partisans often holding exaggerated and inaccurate views of those across the political divide. These misperceptions can lead to ineffective marketing strategies when firms attempt to appeal to broad consumer bases. This study tests whether interacting with an AI chatbot prompted to represent a political outgroup can improve marketers' understanding of that outgroup and reduce affective polarization.

### Hypotheses

We hypothesized that after interacting with an AI chatbot representing their political outgroup, participants would:

1. "**Report more accurate beliefs** about the outgroup's attitudes toward environmentally responsible consumption."
2. "**Report warmer feelings** toward the political outgroup."

## Data

### Data Collection

We collected data via Qualtrics from `r nrow(dat)` participants. Participants self-reported their political orientation on a 0-100 slider scale, where values below 50 indicate Democratic leaning and values 50 and above indicate Republican leaning.

```{r}
#| label: tbl-sample-composition
#| tbl-cap: "Sample composition by learning direction."
tbl <- dat %>%
  count(learner_party) %>%
  mutate(
    learner_party = case_when(
      learner_party == "D→R" ~ "Democrats learning about Republicans (D→R)",
      learner_party == "R→D" ~ "Republicans learning about Democrats (R→D)",
      TRUE ~ learner_party
    )
  ) %>%
  gt() %>%
  cols_label(learner_party = "Learning Direction", n = "N")
save_table(tbl, "tab_sample_composition")
tbl
```

### Measures

We measured two primary outcomes to assess belief updating and affective change.

- **Belief accuracy**: Absolute difference between participants' estimates of outgroup green attitudes and actual outgroup means (lower = more accurate)
- **Outgroup warmth**: Feeling thermometer (0-100 scale) toward the political outgroup

Additionally, we collected confidence in outgroup judgments (5-point scale) and bot informativeness and empathy ratings (5-point scale)

### Pre-Registered Analyses

For each primary outcome, we estimated mixed-effects models of the form:

$$y \sim \text{time} \times \text{learner\_party} + (1 | \text{id})$$

where `time` indexes pre- versus post-interaction measurement and `learner_party` indicates D→R versus R→D learning direction.

## Results

```{r prepare-long-data}
# Prepare long format datasets
accuracy_long <- dat %>%
  select(ResponseId, learner_party, accuracy_pre, accuracy_post) %>%
  pivot_longer(cols = c(accuracy_pre, accuracy_post), names_to = "time",
               names_pattern = "accuracy_(pre|post)", values_to = "error") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

warmth_long <- dat %>%
  select(ResponseId, learner_party, warmth_outgroup_pre, warmth_outgroup_post) %>%
  pivot_longer(cols = c(warmth_outgroup_pre, warmth_outgroup_post), names_to = "time",
               names_pattern = "warmth_outgroup_(pre|post)", values_to = "warmth") %>%
  mutate(time = factor(time, levels = c("pre", "post")))
```

### Preliminaries

Most participants held strong partisan identities, with the majority in the top and bottom quartiles of the political spectrum.

```{r}
#| label: fig-political-orientation
#| fig-cap: "Distribution of political orientation among participants. Values below 50 indicate Democratic leaning, values 50 and above indicate Republican leaning."
#| fig-height: 3
#| fig-width: 4
p <- dat %>%
  ggplot(aes(politicalorientation_1)) +
  geom_histogram(aes(fill = after_stat(x))) +
  labs(x = 'Political Orientation', y = "Count")+
  theme_minimal()+
  theme(legend.position = "none")+
  scale_fill_gradient2(low = party_colors["Democrat"],
                       mid = "gray40",
                       high = party_colors["Republican"],
                       midpoint = 50)  # Adjust midpoint to your scale center
ggsave(paste0(fig_path, "fig_political_orientation.png"), p, width = 4, height = 3, dpi = 300)
p
```

The figure below compares actual green values with perceived ingroup and outgroup attitudes for Democrats and Republicans. The actual gap at baseline was much smaller than participants perceived. Perceptions of the ingroup are much more accurate.

```{r}
#| label: fig-green-attitudes
#| fig-cap: "Actual and perceived environmental attitudes by political party. Panels show participants' own attitudes, their perceptions of ingroup members, and their perceptions of outgroup members before and after the chatbot interaction. Error bars represent standard errors."
#| fig-width: 6
#| fig-height: 4
# Prepare data for faceted plot
prelim_data <- dat %>%
  select(learner_party, participant_party, green_own, green_ingroup, green_outgroup_pre, green_outgroup_post) %>%
  pivot_longer(cols = c(green_own, green_ingroup, green_outgroup_pre, green_outgroup_post),
               names_to = "measure_type", values_to = "value") %>%
  mutate(
    measure_type = factor(measure_type,
                          levels = c("green_own", "green_ingroup", "green_outgroup_pre", "green_outgroup_post"),
                          labels = c("Own Attitudes", "Ingroup Perception", "Outgroup Perception\n(Pre)", "Outgroup Perception\n(Post)")),
    participant_party = factor(participant_party, levels = c("Democrat", "Republican"))
  )

# Create faceted plot
p <- ggplot(prelim_data, aes(x = participant_party, y = value, fill = participant_party)) +
  stat_summary(fun = mean, geom = "bar", alpha = 0.8) +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2) +
  facet_wrap(~measure_type, ncol = 4) +
  scale_fill_manual(values = party_colors, name = "Political Party") +
  labs(
    x = "Political Party",
    y = "Green Attitude (1-5 scale)"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold"),
    legend.position = "none"
  )
ggsave(paste0(fig_path, "fig_green_attitudes.png"), p, width = 6, height = 4, dpi = 300)
p
```

```{r bias}
# Calculate baseline bias comparison
bias_test <- dat %>%
  select(learner_party, accuracy_pre) %>%
  rstatix::t_test(accuracy_pre~learner_party)

p <- bias_test %>% pull(p)

d <- dat %>%
  select(learner_party, accuracy_pre) %>%
  rstatix::cohens_d(accuracy_pre~learner_party) %>% pull(effsize) %>% Ben::numformat()

# Calculate means and SDs by group
bias_descriptives <- dat %>%
  group_by(learner_party) %>%
  summarise(
    mean_bias = mean(accuracy_pre, na.rm = TRUE),
    sd_bias = sd(accuracy_pre, na.rm = TRUE)
  )

mean_dr <- bias_descriptives %>% filter(learner_party == "D→R") %>% pull(mean_bias)
sd_dr <- bias_descriptives %>% filter(learner_party == "D→R") %>% pull(sd_bias)
mean_rd <- bias_descriptives %>% filter(learner_party == "R→D") %>% pull(mean_bias)
sd_rd <- bias_descriptives %>% filter(learner_party == "R→D") %>% pull(sd_bias)
```

What is the distribution of bias? At baseline, Democrats were more biased about Republicans (*M* = `r sprintf("%.2f", mean_dr)`, *SD* = `r sprintf("%.2f", sd_dr)`) than Republicans were about Democrats (*M* = `r sprintf("%.2f", mean_rd)`, *SD* = `r sprintf("%.2f", sd_rd)`), *d* = `r d`, *p* = `r sprintf("%.3f", p)`.

```{r}
#| label: fig-bias-distribution
#| fig-cap: "Distribution of baseline accuracy in estimating outgroup environmental attitudes. Left panel shows overall distribution; right panel shows distribution by political party. Democrats were more biased about Republicans than Republicans were about Democrats."
#| fig-subcap:
#|   - "Overall distribution of baseline accuracy"
#|   - "Baseline accuracy by political party"
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 3

# Overall distribution
p1 <- dat %>%
  ggplot(aes(x = accuracy_pre)) +
  geom_density(color = NA, fill = "gray30", alpha = 0.6) +
  labs(
    x = "Baseline Accuracy (higher = more accurate)",
    y = "Density"
  ) +
  theme_minimal()

# By participant party
p2 <- dat %>%
  ggplot(aes(x = accuracy_pre, fill = participant_party)) +
  geom_density(color = NA, alpha = 0.6) +
  scale_fill_manual(
    values = c("Democrat" = "steelblue", "Republican" = "firebrick"),
    name = NULL
  ) +
  labs(
    x = "Baseline Accuracy (higher = more accurate)",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    legend.position = c(1-0.85, 0.85)
  )

ggsave(paste0(fig_path, "fig_bias_distribution_overall.png"), p1, width = 4, height = 3, dpi = 300)
ggsave(paste0(fig_path, "fig_bias_distribution_by_party.png"), p2, width = 4, height = 3, dpi = 300)
p1
p2
```

Does baseline accuracy correlate with baseline warmth? We examine whether people who are more biased about the outgroup also feel colder toward them.

```{r}
#| label: fig-accuracy-warmth-baseline
#| fig-cap: "Relationship between baseline accuracy and warmth toward political outgroup. Points represent individual participants, lines show linear fits by party. Correlation statistics displayed on plot."
#| fig-width: 6
#| fig-height: 4

p <- dat %>%
  ggplot(aes(x = accuracy_pre, y = warmth_outgroup_pre, color = participant_party)) +
  geom_point(alpha = 0.6, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = party_colors, name = "Political Party") +
  labs(
    x = "Baseline Accuracy (higher = more accurate)",
    y = "Baseline Warmth (0-100 scale)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_baseline_accuracy_warmth.png"), p, width = 6, height = 4, dpi = 300)
p
```

Does baseline accuracy predict baseline confidence? We examine whether less accurate participants are overconfident in their judgments, consistent with Dunning-Kruger effects.

```{r baseline-accuracy-confidence-correlation}
#| label: fig-dunning-kruger
#| fig-cap: "Relationship between skill and confidence in outgroup judgments before and after chatbot interaction. Solid lines show pre-interaction patterns, dashed lines show post-interaction patterns. Less skilled participants (those with lower accuracy) exhibited higher confidence at baseline, consistent with Dunning-Kruger effects. Lines represent LOESS smoothers with 95% confidence bands."
#| fig-width: 8
#| fig-height: 4

# Create inverted accuracy for plotting (higher = better skill)
dat_dk <- dat %>%
  mutate(
    skill_pre = -accuracy_pre,  # Flip so higher = more accurate
    skill_post = -accuracy_post
  ) %>%
  select(ResponseId, participant_party, skill_pre, skill_post,
         confidence_outgroup_pre, confidence_outgroup_post) %>%
  pivot_longer(cols = c(skill_pre, skill_post, confidence_outgroup_pre, confidence_outgroup_post),
               names_to = c(".value", "time"),
               names_pattern = "(.+)_(pre|post)") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

p <- ggplot(dat_dk, aes(x = skill, y = confidence_outgroup, linetype = time, color = participant_party, fill = participant_party)) +
  geom_smooth(method = "loess", se = TRUE, linewidth = 1, alpha = .1) +
  facet_wrap(~participant_party) +
  scale_linetype_manual(values = c("pre" = "solid", "post" = "dashed"),
                        labels = c("pre" = "Pre-interaction", "post" = "Post-interaction"),
                        name = "Time") +
  labs(
    x = "Skill (Higher = More Accurate)",
    y = "Confidence (1-5 scale)"
  ) +
  theme_minimal() +
  scale_color_manual(values = party_colors)+
  scale_fill_manual(values = party_colors)+
  theme(legend.position = "none",
        strip.text = element_text(face = "bold"))
ggsave(paste0(fig_path, "fig_dunning_kruger.png"), p, width = 8, height = 4, dpi = 300)
p
```

### Q1. Do people update their beliefs after interacting with the chatbot?

```{r q1-data-prep}
# Calculate summary statistics for plots
accuracy_summary <- accuracy_long %>%
  group_by(learner_party, time) %>%
  summarise(mean_error = mean(error, na.rm = TRUE),
            se_error = sd(error, na.rm = TRUE) / sqrt(n()),
            .groups = "drop")

warmth_summary <- warmth_long %>%
  group_by(learner_party, time) %>%
  summarise(mean_warmth = mean(warmth, na.rm = TRUE),
            se_warmth = sd(warmth, na.rm = TRUE) / sqrt(n()),
            .groups = "drop")
```



```{r q1-models}
# Estimate mixed-effects models
accuracy_model0 <- lmer(error ~ time  + (1 | ResponseId), data = accuracy_long)
warmth_model0 <- lmer(warmth ~ time  + (1 | ResponseId), data = warmth_long)

accuracy_model <- lmer(error ~ time * learner_party + (1 | ResponseId), data = accuracy_long)
warmth_model <- lmer(warmth ~ time * learner_party + (1 | ResponseId), data = warmth_long)

# Extract coefficients for inline reporting
accuracy_time_coef <- fixef(accuracy_model0)["timepost"]
warmth_time_coef <- fixef(warmth_model0)["timepost"]

# Calculate Cohen's d for main effects
accuracy_d <- cohens_d(error ~ time, data = accuracy_long, pooled_sd = FALSE)$Cohens_d
warmth_d <- cohens_d(warmth ~ time, data = warmth_long, pooled_sd = FALSE)$Cohens_d

```

We examined whether participants changed their beliefs about the outgroup's environmental attitudes from pre- to post-interaction, analyzing both accuracy and warmth. As shown in the table above, participants became significantly more accurate (improving by `r sprintf("%.2f", accuracy_time_coef)` points on the 5-point scale, *d* = `r sprintf("%.2f", accuracy_d)`) and warmer toward the outgroup (increasing by `r sprintf("%.1f", warmth_time_coef)` degrees on the 100-point thermometer, *d* = `r sprintf("%.2f", warmth_d)`).

```{r q1-plots}
#| label: fig-accuracy-warmth-change
#| fig-cap: "Changes in belief accuracy and outgroup warmth from pre- to post-interaction. Left panel shows participants became more accurate in estimating outgroup environmental attitudes. Right panel shows participants reported increased warmth toward the political outgroup on a 0-100 feeling thermometer. Error bars represent standard errors. Both Democrats learning about Republicans (D→R) and Republicans learning about Democrats (R→D) showed significant improvements."
#| fig-width: 10
#| fig-height: 4

# Combine plots side by side using patchwork
p_accuracy <- ggplot(accuracy_summary, aes(x = time, y = mean_error, color = learner_party, group = learner_party)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_error - se_error, ymax = mean_error + se_error), width = 0.1) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(x = "Time", y = "Mean Accuracy") +
  theme_minimal() +
  theme(legend.position = "bottom")

p_warmth <- ggplot(warmth_summary, aes(x = time, y = mean_warmth, color = learner_party, group = learner_party)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_warmth - se_warmth, ymax = mean_warmth + se_warmth), width = 0.1) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(x = "Time", y = "Mean Warmth") +
  theme_minimal() +
  theme(legend.position = "bottom")

p_combined <- p_accuracy + p_warmth + plot_layout(guides = "collect") & theme(legend.position = "bottom")
ggsave(paste0(fig_path, "fig_q1_accuracy_warmth.png"), p_combined, width = 10, height = 4, dpi = 300)
p_combined
```

```{r}
#| label: tbl-main-effects
#| tbl-cap: "Mixed-effects models predicting belief accuracy and outgroup warmth from chatbot interaction timing. Models include random intercepts by participant. Time coefficient represents the main effect of pre- to post-interaction change. Both outcomes showed significant improvement following the chatbot interaction."
# Display models side by side
tbl <- modelsummary(
  list(
    "Accuracy" = accuracy_model0,
    "Warmth" = warmth_model0),
  stars = TRUE,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Accuracy is negative absolute error (higher = more accurate). * p<0.05, ** p<0.01, *** p<0.001"
)
save_table(tbl, "tab_q1_main_effects")
tbl
```

Do changes in accuracy correlate with changes in warmth? We examine whether people who became more accurate also became warmer toward the outgroup.

```{r accuracy-warmth-change-correlation}
#| label: fig-accuracy-warmth-correlation
#| fig-cap: "Correlation between changes in belief accuracy and changes in outgroup warmth. Points represent individual participants, with separate regression lines for Democrats learning about Republicans (D→R) and Republicans learning about Democrats (R→D). Negative values on x-axis indicate improved accuracy. Correlation statistics are displayed separately by learner party."
#| fig-width: 6
#| fig-height: 4

# Create change scores for this analysis (reusing from later section)
dat_with_changes <- dat %>%
  mutate(
    accuracy_change = accuracy_post - accuracy_pre,  # Negative = improvement
    warmth_change = warmth_outgroup_post - warmth_outgroup_pre
  )

p <- dat_with_changes %>%
  ggplot(aes(x = accuracy_change, y = warmth_change, color = learner_party)) +
  geom_point(alpha = 0.6, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Accuracy Change (Negative = Improvement)",
    y = "Warmth Change (Positive = Increase)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_accuracy_warmth_change.png"), p, width = 6, height = 4, dpi = 300)
p
```

### Q2. Is belief updating symmetric across political groups?

```{r}
# Get simple effects
# Note: reverse = TRUE ensures contrasts are computed as "post - pre" to match model coefficients
accuracy_contrasts <- emmeans(accuracy_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

warmth_contrasts <- emmeans(warmth_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Extract values for inline reporting
acc_dr_est <- abs(accuracy_contrasts$estimate[1])  # D→R effect (negative = improvement)
acc_dr_p <- accuracy_contrasts$p.value[1]
acc_rd_est <- abs(accuracy_contrasts$estimate[2])  # R→D effect
acc_rd_p <- accuracy_contrasts$p.value[2]

warmth_dr_est <- warmth_contrasts$estimate[1]  # D→R effect
warmth_dr_p <- warmth_contrasts$p.value[1]
warmth_rd_est <- warmth_contrasts$estimate[2]  # R→D effect
warmth_rd_p <- warmth_contrasts$p.value[2]

# Calculate Cohen's d for simple effects by learner party
acc_dr_d <- cohens_d(error ~ time, data = accuracy_long %>% filter(learner_party == "D→R"), pooled_sd = FALSE)$Cohens_d
acc_rd_d <- cohens_d(error ~ time, data = accuracy_long %>% filter(learner_party == "R→D"), pooled_sd = FALSE)$Cohens_d

warmth_dr_d <- cohens_d(warmth ~ time, data = warmth_long %>% filter(learner_party == "D→R"), pooled_sd = FALSE)$Cohens_d
warmth_rd_d <- cohens_d(warmth ~ time, data = warmth_long %>% filter(learner_party == "R→D"), pooled_sd = FALSE)$Cohens_d

# Get interaction term from model
acc_interaction_coef <- fixef(accuracy_model)["timepost:learner_partyR→D"]
acc_interaction_p <- summary(accuracy_model)$coefficients["timepost:learner_partyR→D", "Pr(>|t|)"]

warmth_interaction_coef <- fixef(warmth_model)["timepost:learner_partyR→D"]
warmth_interaction_p <- summary(warmth_model)$coefficients["timepost:learner_partyR→D", "Pr(>|t|)"]

# Format for modelsummary
format_emmeans <- function(contrasts) {
  contrasts %>%
    mutate(
      stars = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01 ~ "**",
        p.value < 0.05 ~ "*",
        TRUE ~ ""
      ),
      formatted = sprintf("%.3f%s", estimate, stars),
      se_formatted = sprintf("(%.3f)", std.error)
    )
}

acc_formatted <- format_emmeans(accuracy_contrasts)
warmth_formatted <- format_emmeans(warmth_contrasts)

rows = tibble(
    term = c("Time effect (Democrat)", "", "Time effect (Republican)", ""),
    `Accuracy` = c(acc_formatted$formatted[1], acc_formatted$se_formatted[1],
                   acc_formatted$formatted[2], acc_formatted$se_formatted[2]),
    `Warmth` = c(warmth_formatted$formatted[1], warmth_formatted$se_formatted[1],
                 warmth_formatted$formatted[2], warmth_formatted$se_formatted[2])
  )
attr(rows, "position") <- c(1, 2, 3, 4) # Inserts at rows 1 and 2

```

While both groups became more accurate, it was Democrats learning about Republicans who showed greater improvement (*b* = `r sprintf("%.2f", acc_dr_est)`, *d* = `r sprintf("%.2f", acc_dr_d)`, *p* `r if(acc_dr_p < 0.001) "< .001" else paste("=", sprintf("%.3f", acc_dr_p))`). Republicans also became less biased about Democrats, though to a lesser extent (*b* = `r sprintf("%.2f", acc_rd_est)`, *d* = `r sprintf("%.2f", acc_rd_d)`, *p* `r if(acc_rd_p < 0.001) "< .001" else paste("=", sprintf("%.3f", acc_rd_p))`). The interaction term was `r if(acc_interaction_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.2f", acc_interaction_coef)`, *p* `r if(acc_interaction_p < 0.001) "< .001" else paste("=", sprintf("%.3f", acc_interaction_p))`).

Both groups became warmer toward the outgroup at similar rates. Democrats learning about Republicans increased warmth by `r sprintf("%.1f", warmth_dr_est)` degrees (*d* = `r sprintf("%.2f", warmth_dr_d)`, *p* `r if(warmth_dr_p < 0.001) "< .001" else paste("=", sprintf("%.3f", warmth_dr_p))`), while Republicans learning about Democrats increased by `r sprintf("%.1f", warmth_rd_est)` degrees (*d* = `r sprintf("%.2f", warmth_rd_d)`, *p* `r if(warmth_rd_p < 0.001) "< .001" else paste("=", sprintf("%.3f", warmth_rd_p))`). The interaction was `r if(warmth_interaction_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.2f", warmth_interaction_coef)`, *p* `r if(warmth_interaction_p < 0.001) "< .001" else paste("=", sprintf("%.3f", warmth_interaction_p))`).

```{r}
#| label: tbl-symmetry
#| tbl-cap: "Tests of asymmetry in belief updating across political groups. Upper panel shows simple effects of time separately for Democrats learning about Republicans (D→R) and Republicans learning about Democrats (R→D). Lower panel shows full mixed-effects regression results including the Time × Learner Party interaction term. Democrats showed significantly larger accuracy improvements than Republicans, while warmth increases were symmetric across groups."
# Create the table with add_rows
tbl <- modelsummary(
  list("Accuracy" = accuracy_model, "Warmth" = warmth_model),
  stars = TRUE,
  add_rows = rows,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Accuracy is negative absolute error (higher = more accurate). * p<0.05, ** p<0.01, *** p<0.001"
) %>%
 strip_tt(line = TRUE) |>
    group_tt(i = list("Effects by Political Affiliation" = 1, "Full Regression Results" = 5)) |>
      style_tt(i = c(1, 6), bold = TRUE) |>
    style_tt(i = c(14, 16), line = "b", line_color = "lightgray")
save_table(tbl, "tab_q2_interaction")
tbl
```

### Q3. Does political extremity predict how much people update?

```{r extremism-models}
# Add extremism to long data
accuracy_long_ext <- accuracy_long %>%
  left_join(dat %>% select(ResponseId, political_extremism), by = "ResponseId")

warmth_long_ext <- warmth_long %>%
  left_join(dat %>% select(ResponseId, political_extremism), by = "ResponseId")

# First, test extremism predicting baseline outcomes
baseline_accuracy_extremism <- lm(accuracy_pre ~ political_extremism, data = dat)
baseline_warmth_extremism <- lm(warmth_outgroup_pre ~ political_extremism, data = dat)

# Extract coefficients for inline reporting
extremism_acc_baseline_b <- coef(baseline_accuracy_extremism)["political_extremism"]
extremism_acc_baseline_p <- summary(baseline_accuracy_extremism)$coefficients["political_extremism", "Pr(>|t|)"]

extremism_warmth_baseline_b <- coef(baseline_warmth_extremism)["political_extremism"]
extremism_warmth_baseline_p <- summary(baseline_warmth_extremism)$coefficients["political_extremism", "Pr(>|t|)"]

# Calculate correlation-based effect sizes for baseline extremism effects
extremism_acc_baseline_r <- cor(dat$political_extremism, dat$accuracy_pre, use = "complete.obs")
extremism_warmth_baseline_r <- cor(dat$political_extremism, dat$warmth_outgroup_pre, use = "complete.obs")

# Models with extremism interaction
accuracy_extremism_model <- lmer(
  error ~ time * learner_party * political_extremism + (1 | ResponseId),
  data = accuracy_long_ext
)

warmth_extremism_model <- lmer(
  warmth ~ time * learner_party * political_extremism + (1 | ResponseId),
  data = warmth_long_ext
)

# Extract three-way interaction terms for inline reporting
extremism_3way_acc_coef <- fixef(accuracy_extremism_model)["timepost:learner_partyR→D:political_extremism"]
extremism_3way_acc_p <- summary(accuracy_extremism_model)$coefficients["timepost:learner_partyR→D:political_extremism", "Pr(>|t|)"]

extremism_3way_warmth_coef <- fixef(warmth_extremism_model)["timepost:learner_partyR→D:political_extremism"]
extremism_3way_warmth_p <- summary(warmth_extremism_model)$coefficients["timepost:learner_partyR→D:political_extremism", "Pr(>|t|)"]


# Get simple effects at different extremism levels (low = -1 SD, high = +1 SD)
extremism_mean <- mean(accuracy_long_ext$political_extremism, na.rm = TRUE)
extremism_sd <- sd(accuracy_long_ext$political_extremism, na.rm = TRUE)

low_extremism = extremism_mean - extremism_sd
high_extremism = extremism_mean + extremism_sd

accuracy_extremism_contrasts <- emmeans(accuracy_extremism_model,
  ~ time | political_extremism,
  at = list(political_extremism = c(low_extremism, high_extremism))) %>%
  pairs(reverse = TRUE) %>%
  tidy()

warmth_extremism_contrasts <- emmeans(warmth_extremism_model, ~ time | political_extremism,
at = list(political_extremism = c(low_extremism, high_extremism))
) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Format simple effects
acc_ext_formatted <- accuracy_extremism_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

warmth_ext_formatted <- warmth_extremism_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

rows_extremism <- tibble(
  term = c("Time effect (Low Extremism)", "", "Time effect (High Extremism)", ""),
  `Accuracy` = c(acc_ext_formatted$formatted[1], acc_ext_formatted$se_formatted[1],
                 acc_ext_formatted$formatted[2], acc_ext_formatted$se_formatted[2]),
  `Warmth` = c(warmth_ext_formatted$formatted[1], warmth_ext_formatted$se_formatted[1],
               warmth_ext_formatted$formatted[2], warmth_ext_formatted$se_formatted[2])
)
attr(rows_extremism, "position") <- c(1, 2, 3, 4)
```

Politically extreme participants showed greater bias about the outgroup at baseline (*b* = `r sprintf("%.3f", extremism_acc_baseline_b)`, *r* = `r sprintf("%.2f", extremism_acc_baseline_r)`, *p* `r if(extremism_acc_baseline_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_acc_baseline_p))`) and felt less warm toward them (*b* = `r sprintf("%.2f", extremism_warmth_baseline_b)`, *r* = `r sprintf("%.2f", extremism_warmth_baseline_r)`, *p* `r if(extremism_warmth_baseline_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_warmth_baseline_p))`). However, political extremism did not moderate the effect of the chatbot interaction. The three-way interaction for accuracy was `r if(extremism_3way_acc_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.3f", extremism_3way_acc_coef)`, *p* `r if(extremism_3way_acc_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_3way_acc_p))`), and for warmth was `r if(extremism_3way_warmth_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.2f", extremism_3way_warmth_coef)`, *p* `r if(extremism_3way_warmth_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_3way_warmth_p))`). More and less extreme participants were equally likely to update their beliefs and warmth toward the outgroup.

```{r}
#| label: tbl-extremism
#| tbl-cap: "Tests of whether political extremism moderates belief updating. Upper panel shows simple effects of time at low extremism (-1 SD) and high extremism (+1 SD) levels. Lower panel shows full mixed-effects regressions with three-way interactions (Time × Learner Party × Political Extremism). While politically extreme participants showed greater baseline bias, extremism did not significantly moderate the effectiveness of the chatbot intervention."
# Display models
tbl <- modelsummary(
  list("Accuracy" = accuracy_extremism_model, "Warmth" = warmth_extremism_model),
  stars = TRUE,
  add_rows = rows_extremism,
  coef_omit = "Intercept",
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Time effects show pre-to-post change at Low Extremism (-1 SD) and High Extremism (+1 SD). Three-way interaction tests whether extremism moderates differential updating. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  strip_tt(line = TRUE) %>%
  group_tt(i = list("Simple Effects by Extremism Level" = 1, "Full Regression Results" = 5)) %>%
  style_tt(i = c(1, 6), bold = TRUE) %>%
  style_tt(i = c(20, 21), line = "b", line_color = "lightgray")
save_table(tbl, "tab_q3_extremism")
tbl
```

### Q4. Is belief updating associated with how the chatbot is perceived?

```{r process-models}
# Prepare data for regressions
dat_for_reg <- dat %>%
  filter(!is.na(accuracy_pre), !is.na(accuracy_post),
         !is.na(warmth_outgroup_pre), !is.na(warmth_outgroup_post),
         !is.na(info), !is.na(empathy))

# Process measure regressions
accuracy_process_model <- lm(accuracy_post ~ accuracy_pre + info + empathy, data = dat_for_reg)
warmth_process_model <- lm(warmth_outgroup_post ~ warmth_outgroup_pre + info + empathy, data = dat_for_reg)

# Extract coefficients for inline reporting
info_acc_b <- coef(accuracy_process_model)["info"]
info_acc_p <- summary(accuracy_process_model)$coefficients["info", "Pr(>|t|)"]
empathy_acc_b <- coef(accuracy_process_model)["empathy"]
empathy_acc_p <- summary(accuracy_process_model)$coefficients["empathy", "Pr(>|t|)"]

info_warmth_b <- coef(warmth_process_model)["info"]
info_warmth_p <- summary(warmth_process_model)$coefficients["info", "Pr(>|t|)"]
empathy_warmth_b <- coef(warmth_process_model)["empathy"]
empathy_warmth_p <- summary(warmth_process_model)$coefficients["empathy", "Pr(>|t|)"]
```

Participants who found the chatbot more informative showed `r if(info_acc_p < 0.05) "significantly" else "marginally"` better accuracy improvement (*b* = `r sprintf("%.3f", info_acc_b)`, *p* `r if(info_acc_p < 0.001) "< .001" else paste("=", sprintf("%.3f", info_acc_p))`) and `r if(info_warmth_p < 0.05) "significantly" else "marginally"` greater warmth increases (*b* = `r sprintf("%.2f", info_warmth_b)`, *p* `r if(info_warmth_p < 0.001) "< .001" else paste("=", sprintf("%.3f", info_warmth_p))`). Bot empathy did not significantly predict accuracy change (*b* = `r sprintf("%.3f", empathy_acc_b)`, *p* `r if(empathy_acc_p < 0.001) "< .001" else paste("=", sprintf("%.3f", empathy_acc_p))`) or warmth change (*b* = `r sprintf("%.2f", empathy_warmth_b)`, *p* `r if(empathy_warmth_p < 0.001) "< .001" else paste("=", sprintf("%.3f", empathy_warmth_p))`).

```{r}
#| label: tbl-mechanisms
#| tbl-cap: "OLS regressions testing whether chatbot informativeness and empathy predict post-interaction accuracy and warmth, controlling for baseline levels. Bot informativeness significantly predicted both improved accuracy and increased warmth toward the outgroup, while perceived empathy did not significantly predict either outcome."
# Display models
tbl <- modelsummary(
  list("Accuracy" = accuracy_process_model, "Warmth" = warmth_process_model),
  stars = TRUE,
  gof_map = c("nobs", "r.squared", "adj.r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Regressions control for pre-interaction values. * p<0.05, ** p<0.01, *** p<0.001"
)
save_table(tbl, "tab_q4_process")
tbl
```

Do perceptions of informativeness and empathy correlate with each other? Yes, but not strongly (r ~ .30). 

```{r}
#| label: fig-empathy-informativeness
#| fig-cap: "Correlation between perceived chatbot empathy and informativeness. Points are jittered to reduce overplotting. The two dimensions showed modest positive correlations (r approximately .30), suggesting they capture partially distinct aspects of chatbot interaction quality. Correlation statistics displayed separately by learner party."
#| fig-width: 6
#| fig-height: 4
p <- dat_for_reg %>%
  ggplot(aes(x = empathy, y = info, color = learner_party)) +
  geom_jitter(alpha = 0.6, size = 2.5, width = 0.3, height = 0.3) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Empathy Rating (1-5)",
    y = "Informativeness Rating (1-5)"
  ) +
  theme_minimal()+
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_empathy_info.png"), p, width = 6, height = 4, dpi = 300)
p
```

### Q5. Does engagement ("dose") matter for belief updating?

```{r engagement-descriptives}
# Descriptive statistics for engagement
engagement_summary <- dat %>%
  summarise(
    mean_turns = mean(user_turn_count, na.rm = TRUE),
    sd_turns = sd(user_turn_count, na.rm = TRUE),
    mean_words = mean(user_word_count, na.rm = TRUE),
    sd_words = sd(user_word_count, na.rm = TRUE)
  )

# Create change scores and log-transformed word count
# No +1 needed for log transform since min word count = 47
dat <- dat %>%
  mutate(
    accuracy_change = accuracy_post - accuracy_pre,  # Negative = improvement
    warmth_change = warmth_outgroup_post - warmth_outgroup_pre,
    log_word_count = log(user_word_count)
  )
```

We measured engagement as the number of words participants typed during the chat and tested whether greater engagement predicted larger changes in beliefs and warmth. 
Participants engaged in an average of `r sprintf("%.2f", engagement_summary$mean_turns)` conversation turns (*SD* = `r sprintf("%.2f", engagement_summary$sd_turns)`) and typed an average of `r sprintf("%.2f", engagement_summary$mean_words)` words (*SD* = `r sprintf("%.2f", engagement_summary$sd_words)`).

#### Does engagement predict belief accuracy improvement?

```{r engagement-models}
# Model: Engagement predicting accuracy and warmth change
m_accuracy_turns <- lm(accuracy_change ~ user_turn_count + accuracy_pre + learner_party,
                       data = dat)
m_accuracy_log_words <- lm(accuracy_change ~ log_word_count + accuracy_pre + learner_party,
                           data = dat)
m_warmth_turns <- lm(warmth_change ~ user_turn_count + warmth_outgroup_pre + learner_party,
                     data = dat)
m_warmth_log_words <- lm(warmth_change ~ log_word_count + warmth_outgroup_pre + learner_party,
                         data = dat)
```

Greater engagement—measured by turn count—was `r if(coef(summary(m_accuracy_turns))["user_turn_count", "Pr(>|t|)"] < 0.05) "significantly" else "not significantly"` associated with accuracy change (*b* = `r sprintf("%.4f", coef(m_accuracy_turns)["user_turn_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_accuracy_turns))["user_turn_count", "Pr(>|t|)"])`), and `r if(coef(summary(m_warmth_turns))["user_turn_count", "Pr(>|t|)"] < 0.05) "significantly" else "not significantly"` associated with warmth change (*b* = `r sprintf("%.4f", coef(m_warmth_turns)["user_turn_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_warmth_turns))["user_turn_count", "Pr(>|t|)"])`). Using log-transformed word count, the effect on accuracy was `r if(coef(summary(m_accuracy_log_words))["log_word_count", "Pr(>|t|)"] < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.4f", coef(m_accuracy_log_words)["log_word_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_accuracy_log_words))["log_word_count", "Pr(>|t|)"])`), and on warmth was `r if(coef(summary(m_warmth_log_words))["log_word_count", "Pr(>|t|)"] < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.4f", coef(m_warmth_log_words)["log_word_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_warmth_log_words))["log_word_count", "Pr(>|t|)"])`).

```{r}
#| label: tbl-engagement
#| tbl-cap: "OLS regressions testing whether conversation engagement (turn count and log word count) predicts changes in accuracy and warmth, controlling for baseline levels and learner party. Models test whether greater dose of interaction produced larger belief updates. Engagement showed mixed associations with outcomes."
# Consolidated table
tbl <- modelsummary(
  list(
    "Turn Count" = m_accuracy_turns,
    "Log(Word Count)" = m_accuracy_log_words,
    "Turn Count" = m_warmth_turns,
    "Log(Word Count)" = m_warmth_log_words
  ),
  stars = TRUE,
  gof_map = c("nobs", "r.squared", "adj.r.squared"),
  coef_map = coef_map_common
)  %>%
  group_tt(j = list("Accuracy" = 2:3, "Warmth" = 4:5))
save_table(tbl, "tab_q5_engagement")
tbl
```

```{r engagement-plots}
#| label: fig-engagement
#| fig-cap: "Relationships between conversation engagement and belief updating. Top row shows associations between turn count and changes in accuracy (left) and warmth (right). Bottom row shows associations between log-transformed word count and the same outcomes. Points represent individual participants, with outliers exceeding 75 turns excluded. Correlation statistics and linear regression lines displayed separately by learner party. Negative values on accuracy change indicate improvement."
#| fig-width: 10
#| fig-height: 8

# Filter out outliers with over 75 turns
dat_filtered <- dat %>% filter(user_turn_count <= 75)

# Plots with turn count
p1 <- ggplot(dat_filtered, aes(x = user_turn_count, y = accuracy_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  stat_cor()+
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Turn Count",
    y = "Accuracy Change\n(Negative = Improvement)"
  ) +
  theme_minimal()

p2 <- ggplot(dat_filtered, aes(x = user_turn_count, y = warmth_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  stat_cor()+
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Turn Count",
    y = "Warmth Change\n(Positive = Increase)"
  ) +
  theme_minimal()

# Plots with log word count
p3 <- ggplot(dat_filtered, aes(x = log_word_count, y = accuracy_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  stat_cor()+
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Log(Word Count)",
    y = "Accuracy Change\n(Negative = Improvement)"
  ) +
  theme_minimal()

p4 <- ggplot(dat_filtered, aes(x = log_word_count, y = warmth_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  stat_cor()+
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Log(Word Count)",
    y = "Warmth Change\n(Positive = Increase)"
  ) +
  theme_minimal()

# Combine plots with single shared legend at bottom
p_combined <- (p1 + p2) / (p3 + p4) +
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom")
ggsave(paste0(fig_path, "fig_engagement.png"), p_combined, width = 10, height = 8, dpi = 300)
p_combined
```

### Q6. Does interacting with the chatbot improve applied judgment?

Participants wrote slogans for marketing energy-saving appliances to the political outgroup. I haven't touched this data yet, because we don't have a control condition or a baseline slogan to make a meaningful comparison to.

**DATA AVAILABLE**: Raw slogan text in variable `slogan` (n = `r sum(!is.na(dat$slogan))` responses).

**DATA NEEDED**:
1. Semantic similarity scores (compare to AI-generated responses to detect copying)
2. Quality ratings (human or LLM ratings of slogan fit to target outgroup)

### Q7. Does confidence track accuracy—or diverge from it?

We examine participants' metacognitive awareness by testing (1) whether confidence changes from pre to post, and (2) whether confidence changes track with actual accuracy improvements.

#### Does confidence change pre-to-post?

```{r confidence-model}
# Prepare confidence data in long format
confidence_long <- dat %>%
  select(ResponseId, learner_party, confidence_outgroup_pre, confidence_outgroup_post) %>%
  pivot_longer(cols = c(confidence_outgroup_pre, confidence_outgroup_post),
               names_to = "time", names_pattern = "confidence_outgroup_(pre|post)",
               values_to = "confidence") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

# Calculate summary statistics for plot
confidence_summary <- confidence_long %>%
  group_by(learner_party, time) %>%
  summarise(mean_confidence = mean(confidence, na.rm = TRUE),
            se_confidence = sd(confidence, na.rm = TRUE) / sqrt(n()),
            .groups = "drop")



# Mixed-effects models
confidence_model0 <- lmer(confidence ~ time + (1 | ResponseId), data = confidence_long)
confidence_model <- lmer(confidence ~ time * learner_party + (1 | ResponseId), data = confidence_long)

# Get simple effects for the interaction model
confidence_contrasts <- emmeans(confidence_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Extract values for inline reporting
conf_dr_est <- confidence_contrasts$estimate[1]  # D→R effect
conf_dr_p <- confidence_contrasts$p.value[1]
conf_rd_est <- confidence_contrasts$estimate[2]  # R→D effect
conf_rd_p <- confidence_contrasts$p.value[2]

# Calculate Cohen's d for confidence changes by learner party
conf_dr_d <- cohens_d(confidence ~ time, data = confidence_long %>% filter(learner_party == "D→R"), pooled_sd = FALSE)$Cohens_d
conf_rd_d <- cohens_d(confidence ~ time, data = confidence_long %>% filter(learner_party == "R→D"), pooled_sd = FALSE)$Cohens_d

# Format simple effects
conf_formatted <- confidence_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

rows_confidence <- tibble(
  term = c("Time effect (D→R)", "", "Time effect (R→D)", ""),
  `Confidence (Main Effect)` = c("", "", "", ""),
  `Confidence (Interaction)` = c(conf_formatted$formatted[1], conf_formatted$se_formatted[1],
                                 conf_formatted$formatted[2], conf_formatted$se_formatted[2])
)
attr(rows_confidence, "position") <- c(1, 2, 3, 4)
```

Republicans learning about Democrats gained confidence after the interaction (*b* = `r sprintf("%.3f", conf_rd_est)`, *d* = `r sprintf("%.2f", conf_rd_d)`, *p* `r if(conf_rd_p < 0.001) "< .001" else paste("=", sprintf("%.3f", conf_rd_p))`), despite learning less than Democrats. Democrats' confidence did not significantly change (*b* = `r sprintf("%.3f", conf_dr_est)`, *d* = `r sprintf("%.2f", conf_dr_d)`, *p* `r if(conf_dr_p < 0.001) "< .001" else paste("=", sprintf("%.3f", conf_dr_p))`).

```{r}
#| label: fig-confidence-change
#| fig-cap: "Changes in confidence in outgroup judgments from pre- to post-interaction. Republicans learning about Democrats showed significant increases in confidence, while Democrats learning about Republicans showed no significant change. Error bars represent standard errors."
#| fig-width: 6
#| fig-height: 4
# Confidence plot
p <- ggplot(confidence_summary, aes(x = time, y = mean_confidence, color = learner_party, group = learner_party)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_confidence - se_confidence, ymax = mean_confidence + se_confidence), width = 0.1) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(x = "Time", y = "Mean Confidence (1-5)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_confidence.png"), p, width = 6, height = 4, dpi = 300)
p

```{r}
#| label: tbl-confidence
#| tbl-cap: "Mixed-effects models testing changes in confidence in outgroup judgments. Upper panel shows simple effects of time separately by learner party (D→R and R→D). Lower panel shows full regression results with Time × Learner Party interaction. Republicans gained confidence after the interaction, while Democrats did not, suggesting asymmetric metacognitive responses."
# Display models
tbl <- modelsummary(
  list("Confidence (Main Effect)" = confidence_model0,
       "Confidence (Interaction)" = confidence_model),
  stars = TRUE,
  add_rows = rows_confidence,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Confidence rated on 1-5 scale. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  strip_tt(line = TRUE) %>%
  group_tt(i = list("Simple Effects by Political Affiliation" = 1, "Full Regression Results" = 5)) %>%
  style_tt(i = c(1, 6), bold = TRUE) %>%
  style_tt(i = c(14, 16), line = "b", line_color = "lightgray")
save_table(tbl, "tab_q7_confidence")
tbl
```

#### Correlation: Does confidence change track accuracy improvement?

```{r confidence-calibration, fig.width=8, fig.height=6}
# Compute change scores
calibration_dat <- dat_for_reg %>%
  mutate(
    confidence_change = confidence_outgroup_post - confidence_outgroup_pre,
    accuracy_change = accuracy_post - accuracy_pre  # Negative = improvement
  )

# Compute correlations by group
cor_overall <- cor.test(calibration_dat$confidence_change, calibration_dat$accuracy_change)
cor_dr <- cor.test(
  calibration_dat$confidence_change[calibration_dat$learner_party == "D→R"],
  calibration_dat$accuracy_change[calibration_dat$learner_party == "D→R"]
)
cor_rd <- cor.test(
  calibration_dat$confidence_change[calibration_dat$learner_party == "R→D"],
  calibration_dat$accuracy_change[calibration_dat$learner_party == "R→D"]
)
```

Overall, confidence change did not track with accuracy improvement (*r* = `r sprintf("%.3f", cor_overall$estimate)`, *p* `r if(cor_overall$p.value < 0.001) "< .001" else paste("=", sprintf("%.3f", cor_overall$p.value))`). For Democrats learning about Republicans, there was a `r if(cor_dr$p.value < 0.05) "significant positive" else "positive"` correlation between confidence change and accuracy change (*r* = `r sprintf("%.3f", cor_dr$estimate)`, *p* `r if(cor_dr$p.value < 0.001) "< .001" else paste("=", sprintf("%.3f", cor_dr$p.value))`), indicating that those who became less accurate gained more confidence. For Republicans learning about Democrats, confidence change did not correlate with accuracy change (*r* = `r sprintf("%.3f", cor_rd$estimate)`, *p* `r if(cor_rd$p.value < 0.001) "< .001" else paste("=", sprintf("%.3f", cor_rd$p.value))`).

```{r}
#| label: fig-confidence-calibration
#| fig-cap: "Metacognitive calibration showing relationship between confidence change and accuracy change. Points represent individual participants with jittering to reduce overplotting. Quadrant labels indicate well-calibrated (rightfully more/less confident) versus miscalibrated (wrongly more/less confident) changes. Negative values on y-axis indicate accuracy improvement. For Democrats (D→R), increased confidence was paradoxically associated with becoming less accurate, suggesting poor metacognitive calibration. Dashed lines mark no-change thresholds."
#| fig-width: 8
#| fig-height: 6
# Create scatter plot with correlation statistics
p <- ggplot(calibration_dat, aes(x = confidence_change, y = accuracy_change, color = learner_party)) +
  geom_jitter(alpha = 0.6, size = 2.5, width = .2, height= 0) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Confidence Change (Post - Pre)",
    y = "Accuracy Change (Post - Pre)\nNegative = Improvement"
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5)+
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5)+
  annotate(geom = 'text', x = 2.5, y = -2.5, label= 'Rightfully\nmore confident', size = 3) +
  annotate(geom = 'text', x = -2.5, y = -2.5, label= 'Rightfully\nless confident', size = 3) +
  annotate(geom = 'text', x = 2.5, y = 2.5, label= 'Wrongly\nmore confident', size = 3) +
  annotate(geom = 'text', x = -2.5, y = 2.5, label= 'Wrongly\nless confident', size = 3) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
ggsave(paste0(fig_path, "fig_confidence_calibration.png"), p, width = 8, height = 6, dpi = 300)
p
```


## Discussion

### Summary of Findings

Brief interactions with an AI chatbot representing a political outgroup improved accuracy of beliefs about that outgroup and increased warmth toward them. Participants became more accurate in estimating their political outgroup's environmental attitudes and reported warmer feelings toward the outgroup on a feeling thermometer. These effects were asymmetric: Democrats learning about Republicans showed greater accuracy improvements than Republicans learning about Democrats, though both groups increased warmth at similar rates.

Political extremism predicted greater baseline bias and lower baseline warmth but did not moderate the intervention's effectiveness. More and less extreme participants benefited equally from the interaction. Perceptions of the chatbot's informativeness predicted better outcomes, while perceived empathy did not. Greater engagement, measured by conversation length, showed mixed associations with belief updating.

Confidence changes did not track with accuracy improvements. Republicans learning about Democrats gained confidence despite smaller accuracy gains, while Democrats' confidence did not change. For Democrats, confidence increases were paradoxically associated with becoming less accurate, suggesting poor metacognitive calibration in this context.

### Implications for Marketing Practice

These findings suggest that AI-mediated interactions can serve as a scalable tool for reducing political misperceptions in consumer research. Marketers often struggle to understand ideologically diverse consumer segments, relying on stereotypes or costly focus groups. AI chatbots could provide an accessible method for gaining insight into outgroup consumer preferences and values.

This approach may be particularly valuable for marketing environmentally responsible products, where political divides can lead to misperceptions about consumer attitudes. Understanding that political outgroups may be more environmentally conscious than stereotypes suggest could inform more effective cross-partisan messaging strategies.

### Limitations

This study has several limitations that provide directions for future research.

**Domain specificity.** The environmental attitude domain may not generalize to other consumer preferences or policy areas. Environmental attitudes represent a domain where partisan divides exist but are not as stark as other politically charged topics. Future research should examine whether AI-mediated learning transfers to domains with deeper partisan disagreement, such as immigration, healthcare, or gun policy. The effectiveness of this intervention may vary depending on the perceived relevance of the attitude domain to partisan identity.

**Temporal dynamics.** This study examines only immediate post-interaction effects. The durability of accuracy improvements and warmth increases remains unknown. Research on intergroup contact suggests that initial attitude changes can decay over time, particularly without repeated exposure. Future studies should include delayed follow-up measurements to assess whether belief updates persist days, weeks, or months after the interaction. Understanding decay rates would inform practical recommendations about intervention frequency for sustained effects.

**AI representation versus human reality.** Participants interacted with an AI chatbot prompted to represent a political outgroup rather than actual outgroup members. While this approach offers scalability advantages, it raises questions about whether insights gained from AI interactions transfer to understanding real outgroup members. The chatbot was prompted to represent the outgroup but may not fully capture the diversity of views within each political group. Individual variation within partisan groups is substantial, and the AI's responses may reinforce stereotypes about group homogeneity even as they correct specific attitude misperceptions. Future research should examine whether learning from AI translates to more accurate perceptions of actual individuals and whether AI-mediated learning produces different outcomes than human-mediated contact.

**Comparison to alternative interventions.** This study does not compare the chatbot intervention to other approaches for reducing partisan misperceptions. Standard perspective-taking interventions have shown some effectiveness in reducing intergroup bias in the psychological literature. Information provision through simple fact sheets or Google searches might achieve similar accuracy improvements at lower cost. Interactions with digital twins based on actual individuals' response patterns could combine scalability with greater authenticity. Future research should directly compare these alternatives to determine the relative effectiveness and cost-efficiency of AI chatbots versus other scalable interventions for partisan debiasing.

## Appendix

### Correlation Matrix

```{r}
#| label: tbl-correlation-matrix
#| tbl-cap: "Correlation matrix with significance stars for all key variables. Lower triangle shows Pearson correlation coefficients with significance indicators (* p<0.05, ** p<0.01, *** p<0.001). Upper triangle intentionally left blank for readability. Variables include political orientation, belief accuracy, environmental attitudes, outgroup warmth, confidence, bot perceptions, and engagement measures."

# Function to compute correlation with significance stars
cor_with_stars <- function(x, y) {
  if (sum(!is.na(x) & !is.na(y)) < 3) return("")  # Need at least 3 observations
  test <- cor.test(x, y, use = "pairwise.complete.obs")
  r <- test$estimate
  p <- test$p.value

  stars <- case_when(
    p < 0.001 ~ "***",
    p < 0.01 ~ "**",
    p < 0.05 ~ "*",
    TRUE ~ ""
  )

  sprintf("%.2f%s", r, stars)
}

# Select key continuous variables for correlation matrix
cor_vars <- dat %>%
  select(
    # Political orientation
    `Political Orientation` = politicalorientation_1,
    `Political Extremism` = political_extremism,

    # Accuracy measures
    `Accuracy (Pre)` = accuracy_pre,
    `Accuracy (Post)` = accuracy_post,

    # Green attitudes
    `Green Own` = green_own,
    `Green Ingroup` = green_ingroup,
    `Green Outgroup (Pre)` = green_outgroup_pre,
    `Green Outgroup (Post)` = green_outgroup_post,

    # Warmth measures
    `Warmth Outgroup (Pre)` = warmth_outgroup_pre,
    `Warmth Outgroup (Post)` = warmth_outgroup_post,
    `Warmth Ingroup (Pre)` = warmth_ingroup_pre,
    `Warmth Ingroup (Post)` = warmth_ingroup_post,

    # Confidence measures
    `Confidence Ingroup` = confidence_ingroup,
    `Confidence Outgroup (Pre)` = confidence_outgroup_pre,
    `Confidence Outgroup (Post)` = confidence_outgroup_post,

    # Bot perceptions
    `Bot Informativeness` = info,
    `Bot Empathy` = empathy,

    # Engagement
    `Turn Count` = user_turn_count,
    `Word Count` = user_word_count
  )

# Create correlation matrix with stars
n_vars <- ncol(cor_vars)
var_names <- names(cor_vars)
cor_matrix <- matrix("", nrow = n_vars, ncol = n_vars)
rownames(cor_matrix) <- var_names
colnames(cor_matrix) <- var_names

# Fill lower triangle with correlations and stars
for (i in 1:n_vars) {
  for (j in 1:n_vars) {
    if (i == j) {
      cor_matrix[i, j] <- "—"  # Diagonal
    } else if (i > j) {
      cor_matrix[i, j] <- cor_with_stars(cor_vars[[j]], cor_vars[[i]])
    }
    # Upper triangle stays empty
  }
}

# Convert to data frame and create gt table
cor_df <- as.data.frame(cor_matrix)
cor_df <- cbind(Variable = rownames(cor_df), cor_df)
rownames(cor_df) <- NULL

# Create gt table
tbl <- cor_df %>%
  gt() %>%
  tab_header(
    title = "Correlation Matrix of Key Variables"
  ) %>%
  cols_label(
    Variable = ""
  ) %>%
  tab_footnote(
    footnote = "* p<0.05, ** p<0.01, *** p<0.001. Lower triangle shows correlations with significance stars. Upper triangle left blank for readability."
  ) %>%
  tab_style(
    style = cell_text(size = px(10)),
    locations = cells_body()
  ) %>%
  tab_style(
    style = cell_text(size = px(10), weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(size = px(10), weight = "bold"),
    locations = cells_body(columns = Variable)
  )

save_table(tbl, "tab_correlation_matrix")
tbl
```

### Summary table of regressions

```{r}
#| label: tbl-appendix-accuracy-summary
#| tbl-cap: "Consolidated summary of all accuracy models. Columns are grouped by research question: Pre-Post Change (main effect and interaction with learner party), Extremism (moderation by political extremism), and Mechanisms (bot perceptions and engagement dosage). Models 1-3 are mixed-effects with random intercepts by participant; Models 4-6 are OLS with baseline controls."
# Accuracy models panel
tbl_acc <- modelsummary(
  list(
    "Main Effect" = accuracy_model0,
    "Interaction" = accuracy_model,
    "Extremism" = accuracy_extremism_model,
    # "Baseline" = baseline_accuracy_extremism,
    "Information/Empathy" = accuracy_process_model,
    "Turns" = m_accuracy_turns,
    "Words" = m_accuracy_log_words
  ),
  stars = TRUE,
  gof_map = c("nobs", "r.squared", "adj.r.squared"),
  coef_omit = "Intercept",
  statistic = NULL,
  coef_map = coef_map_common,
  notes = "Note: Models 1-3 are mixed effects with random intercepts. Models 4-6 are OLS. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  group_tt(j = list(
    "Pre-Post Change" = 2:3,
    "Extremism" = 4,
    "Mechanisms" = 5:7
  ))
save_table(tbl_acc, "tab_appendix_accuracy_summary")
tbl_acc

```{r}
#| label: tbl-appendix-warmth-summary
#| tbl-cap: "Consolidated summary of all warmth models. Columns are grouped by research question: Pre-Post Change (main effect and interaction with learner party), Extremism (moderation by political extremism), and Mechanisms (bot perceptions and engagement dosage). Models 1-3 are mixed-effects with random intercepts by participant; Models 4-6 are OLS with baseline controls."
# Warmth models panel
tbl_warmth <- modelsummary(
  list(
    "Main Effect" = warmth_model0,
    "Interaction" = warmth_model,
    "Extremism" = warmth_extremism_model,
    "Information/Empathy" = warmth_process_model,
    "Turns" = m_warmth_turns,
    "Words" = m_warmth_log_words
  ),
  stars = TRUE,
  statistic = NULL,
  gof_map = c("nobs", "r.squared", "adj.r.squared"),
  coef_omit = "Intercept",
  coef_map = coef_map_common,
  notes = "Note: Models 1-3 are mixed effects with random intercepts. Models 4-6 are OLS. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  group_tt(j = list(
    "Pre-Post Change" = 2:3,
    "Extremism" = 4,
    "Mechanisms" = 5:7
  ))
save_table(tbl_warmth, "tab_appendix_warmth_summary")
tbl_warmth
```

### Descriptive Statistics

```{r descriptives-actual-attitudes}
#| label: tbl-descriptives-green
#| tbl-cap: "Actual environmental attitudes by political party. Self-reported green attitudes measured on a 1-5 scale, with higher values indicating more pro-environmental attitudes. Democrats and Republicans showed relatively similar baseline environmental values."
# Actual environmental attitudes by party
dat %>%
  group_by(participant_party) %>%
  summarise(
    mean_green = mean(green_own, na.rm = TRUE),
    sd_green = sd(green_own, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    participant_party = "Political Party",
    mean_green = "Mean Green Attitude",
    sd_green = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_green, sd_green), decimals = 2)
```

```{r descriptives-accuracy}
#| label: tbl-descriptives-accuracy
#| tbl-cap: "Outgroup belief accuracy by learner party and time point. Values represent absolute error between participants' estimates of outgroup green attitudes and actual outgroup means, with lower values indicating more accurate beliefs. Both groups showed improvements from pre- to post-interaction."
# Accuracy by time and learner party
accuracy_long %>%
  group_by(learner_party, time) %>%
  summarise(
    mean_error = mean(error, na.rm = TRUE),
    sd_error = sd(error, na.rm = TRUE),
    n = sum(!is.na(error)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    time = "Time",
    mean_error = "Mean Error",
    sd_error = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_error, sd_error), decimals = 3)
```

```{r descriptives-warmth}
#| label: tbl-descriptives-warmth
#| tbl-cap: "Outgroup warmth by learner party and time point. Warmth measured using a feeling thermometer on a 0-100 scale, with higher values indicating warmer feelings toward the political outgroup. Both groups showed increases in warmth from pre- to post-interaction."
# Warmth by time and learner party
warmth_long %>%
  group_by(learner_party, time) %>%
  summarise(
    mean_warmth = mean(warmth, na.rm = TRUE),
    sd_warmth = sd(warmth, na.rm = TRUE),
    n = sum(!is.na(warmth)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    time = "Time",
    mean_warmth = "Mean Warmth",
    sd_warmth = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_warmth, sd_warmth), decimals = 2)
```

```{r descriptives-extremism}
#| label: tbl-descriptives-extremism
#| tbl-cap: "Political extremism by learner party. Extremism operationalized as absolute distance from the political center (50 on the 0-100 political orientation scale), ranging from 0 (centrist) to 50 (maximum extremism). Both Democrats and Republicans showed similar levels of extremism in this sample."
# Political extremism by learner party
dat %>%
  group_by(learner_party) %>%
  summarise(
    mean_extremism = mean(political_extremism, na.rm = TRUE),
    sd_extremism = sd(political_extremism, na.rm = TRUE),
    min_extremism = min(political_extremism, na.rm = TRUE),
    max_extremism = max(political_extremism, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    mean_extremism = "Mean",
    sd_extremism = "SD",
    min_extremism = "Min",
    max_extremism = "Max"
  ) %>%
  fmt_number(columns = c(mean_extremism, sd_extremism, min_extremism, max_extremism), decimals = 2)
```

```{r descriptives-bot-perceptions}
#| label: tbl-descriptives-bot
#| tbl-cap: "Chatbot perception ratings by learner party. Participants rated the chatbot's informativeness (how much they learned) and empathy (how empathetic it was) on 1-5 scales immediately after the interaction. Ratings were generally positive across both dimensions and political groups."
# Bot perceptions by learner party
dat %>%
  select(learner_party, info, empathy) %>%
  pivot_longer(cols = c(info, empathy), names_to = "measure", values_to = "rating") %>%
  mutate(measure = case_when(
    measure == "info" ~ "Informativeness",
    measure == "empathy" ~ "Empathy",
    TRUE ~ measure
  )) %>%
  group_by(learner_party, measure) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    n = sum(!is.na(rating)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    measure = "Measure",
    mean_rating = "Mean Rating",
    sd_rating = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_rating, sd_rating), decimals = 2)
```

```{r descriptives-confidence}
#| label: tbl-descriptives-confidence
#| tbl-cap: "Confidence in outgroup judgments by learner party and time point. Confidence measured on a 1-5 scale asking how confident participants felt about their understanding of the political outgroup. Republicans showed increases in confidence from pre- to post-interaction, while Democrats did not."
# Confidence by time and learner party
dat %>%
  select(ResponseId, learner_party, confidence_outgroup_pre, confidence_outgroup_post) %>%
  pivot_longer(cols = c(confidence_outgroup_pre, confidence_outgroup_post),
               names_to = "time", names_pattern = "confidence_outgroup_(pre|post)",
               values_to = "confidence") %>%
  mutate(time = factor(time, levels = c("pre", "post"))) %>%
  group_by(learner_party, time) %>%
  summarise(
    mean_confidence = mean(confidence, na.rm = TRUE),
    sd_confidence = sd(confidence, na.rm = TRUE),
    n = sum(!is.na(confidence)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    time = "Time",
    mean_confidence = "Mean Confidence",
    sd_confidence = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_confidence, sd_confidence), decimals = 2)
```

### Robustness Checks

#### Warmth Difference Score

Following common practice in the affective polarization literature, we examine warmth using a difference score (outgroup - ingroup). Effects replicate what is reported on the main text.

```{r warmth-difference}
# Prepare warmth difference data
warmth_diff_long <- dat %>%
  select(ResponseId, learner_party, warmth_diff_pre, warmth_diff_post) %>%
  pivot_longer(cols = c(warmth_diff_pre, warmth_diff_post), names_to = "time",
               names_pattern = "warmth_diff_(pre|post)", values_to = "warmth_diff") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

# Mixed-effects model
warmth_diff_model <- lmer(warmth_diff ~ time * learner_party + (1 | ResponseId), data = warmth_diff_long)

# Get simple effects
warmth_diff_contrasts <- emmeans(warmth_diff_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Format simple effects
warmth_diff_formatted <- warmth_diff_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

rows_warmth_diff <- tibble(
  term = c("Time effect (D→R)", "", "Time effect (R→D)", ""),
  `Warmth Difference` = c(warmth_diff_formatted$formatted[1], warmth_diff_formatted$se_formatted[1],
                          warmth_diff_formatted$formatted[2], warmth_diff_formatted$se_formatted[2])
)
attr(rows_warmth_diff, "position") <- c(1, 2, 3, 4)
```

```{r}
#| label: tbl-robustness-warmth-diff
#| tbl-cap: "Robustness check using warmth difference scores (outgroup minus ingroup warmth). This operationalization follows standard practice in affective polarization research. Upper panel shows simple effects by learner party. Lower panel shows full mixed-effects regression. Results replicate main findings, confirming that participants became less affectively polarized toward the political outgroup."
# Display model
modelsummary(
  list("Warmth Difference" = warmth_diff_model),
  stars = TRUE,
  add_rows = rows_warmth_diff,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Warmth difference = outgroup - ingroup. Positive values indicate reduced polarization. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  strip_tt(line = TRUE) %>%
  group_tt(i = list("Simple Effects by Political Affiliation" = 1, "Full Regression Results" = 5)) %>%
  style_tt(i = c(1, 6), bold = TRUE) %>%
  style_tt(i = c(12, 14), line = "b", line_color = "lightgray")
```

### Extreme Cases Analysis

To understand what characterizes successful versus unsuccessful learning, we examined participants who showed the most and least improvement in accuracy about their political outgroup.

```{r extreme-cases-analysis}
#| label: tbl-extreme-cases
#| tbl-cap: "Comparison of participants showing the most versus least improvement in accuracy. For each learner party, we identified the single participant who improved most and the single participant who worsened most (or improved least). Positive values indicate improvement (reduced error). Engagement metrics (word count, turn count) did not consistently distinguish successful from unsuccessful learners, suggesting quality of engagement matters more than quantity."
# Calculate accuracy improvement (negative = improvement)
extreme_cases <- dat %>%
  mutate(accuracy_improvement = accuracy_pre - accuracy_post) %>%
  group_by(learner_party) %>%
  arrange(desc(accuracy_improvement)) %>%
  slice(c(1, n())) %>%
  ungroup() %>%
  mutate(
    case_type = rep(c("Most Improved", "Least Improved"), 2)
  ) %>%
  select(ResponseId, learner_party, case_type,
         accuracy_pre, accuracy_post, accuracy_improvement,
         user_word_count, user_turn_count, slogan)

# Display summary table
extreme_cases %>%
  select(learner_party, case_type, accuracy_improvement, user_word_count, user_turn_count) %>%
  gt() %>%
  cols_label(
    learner_party = "Group",
    case_type = "Case",
    accuracy_improvement = "Accuracy Improvement",
    user_word_count = "Word Count",
    user_turn_count = "Turn Count"
  ) %>%
  fmt_number(columns = accuracy_improvement, decimals = 3) %>%
  fmt_number(columns = c(user_word_count, user_turn_count), decimals = 0) %>%
  tab_footnote(
    footnote = "Positive values indicate improvement (reduced error)",
    locations = cells_column_labels(columns = accuracy_improvement)
  )
```

#### Marketing Messages from Extreme Cases

Participants wrote marketing slogans for energy-saving appliances targeted at their political outgroup. The messages reveal stark differences between successful and unsuccessful learners.

**Democrats Learning About Republicans (D→R)**

*Most Improved* (accuracy improvement = 2.843):
> "Energy-saving Appliances, Helps Make America Great!"

This message uses patriotic language that bridges partisan values. The participant engaged deeply on economic issues and found genuine common ground.

*Least Improved* (accuracy worsened by 1.333):
> "made for independence 'lower your costs, stay in control'."

This message shows confused framing about independence and control. The participant's conversation remained surface-level and distracted.

**Republicans Learning About Democrats (R→D)**

*Most Improved* (accuracy improvement = 3.000):
> "energy saving appliances . save energy and our planet"

This message authentically reflects Democratic environmental values with simple, accessible language. The participant's conversation explored multiple policy areas comprehensively.

*Least Improved* (accuracy worsened by 2.167):
> "Come spend 50% more on less practical appliances since that's what you claim to do and support the business. But I know you won't because just like every Republican said its not financially plausible and your actions only prove the Republicans point."

This openly hostile message reinforces stereotypes rather than bridging divides. The participant challenged the chatbot's authenticity throughout and remained adversarial.

#### Patterns in Successful vs. Unsuccessful Learning

```{r extreme-cases-engagement}
#| label: tbl-extreme-engagement
#| tbl-cap: "Engagement levels comparing successful versus unsuccessful learners among extreme cases. Based on n=4 participants (2 improved, 2 worsened). Participants who worsened actually showed slightly higher engagement on average, reinforcing that conversation quality matters more than quantity for belief updating."
# Compare engagement between successful and unsuccessful learners
engagement_comparison <- extreme_cases %>%
  mutate(
    success = ifelse(accuracy_improvement > 0, "Improved", "Worsened")
  ) %>%
  group_by(success) %>%
  summarise(
    mean_word_count = mean(user_word_count, na.rm = TRUE),
    mean_turn_count = mean(user_turn_count, na.rm = TRUE),
    .groups = "drop"
  )

engagement_comparison %>%
  gt() %>%
  cols_label(
    success = "Outcome",
    mean_word_count = "Mean Word Count",
    mean_turn_count = "Mean Turn Count"
  ) %>%
  fmt_number(columns = c(mean_word_count, mean_turn_count), decimals = 1)
```

The qualitative analysis of conversations (full transcripts available in data/processed/conversations/) reveals distinct patterns:

**Successful learners**:

- Approached conversations with genuine curiosity
- Explored substantive policy topics in depth
- Found common ground on shared concerns
- Asked respectful, probing questions
- Created simple, bridge-building marketing messages

**Unsuccessful learners**:

- Remained skeptical or hostile throughout
- Brought external "evidence" to confirm preconceptions
- Challenged the chatbot's authenticity
- Stayed in adversarial mode
- Created divisive or sarcastic marketing messages

These patterns suggest that genuine engagement is crucial for learning across partisan lines. Participants who used the conversation to confirm existing biases actually became less accurate, while those who approached it with openness showed dramatic improvements.

### Full Conversation Transcripts

The following sections present the complete conversations for each extreme case, illustrating the qualitative differences between successful and unsuccessful learning.

#### D→R Most Improved: R_3smKFSFIo8Nlgir

**Accuracy improvement**: 2.843 (from 2.921 to 0.079)
**Marketing message**: "Energy-saving Appliances, Helps Make America Great!"

```{r conversation-dr-most, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_3smKFSFIo8Nlgir.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This conversation shows deep engagement on economic issues. The participant found genuine common ground on concerns about taxes, regulations, and job stability. The chatbot explored practical implications of reduced government intervention, and the participant asked probing questions about implementation. This substantive exchange led to dramatically improved accuracy and a patriotic marketing message that bridges partisan values.

#### D→R Least Improved: R_7roKHi99S0lQCJq

**Accuracy change**: -1.333 (worsened from 1.088 to 2.421)
**Marketing message**: "made for independence 'lower your costs, stay in control'."

```{r conversation-dr-least, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_7roKHi99S0lQCJq.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This participant appeared distracted (mentioning grading quizzes) and the conversation remained superficial. While practical topics were discussed (appliances, thermostats), the exchange never went deep enough to challenge preconceptions. The participant's attention was divided, and they didn't engage with substantive policy questions. The marketing message reflects this confused engagement.

#### R→D Most Improved: R_7rne5ptEvs5r7Il

**Accuracy improvement**: 3.000 (from 3.251 to 0.251)
**Marketing message**: "energy saving appliances . save energy and our planet"

```{r conversation-rd-most, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_7rne5ptEvs5r7Il.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This wide-ranging conversation explored Democratic values comprehensively—environment, healthcare, social justice, immigration. The participant asked about core beliefs and policy positions, maintaining a respectful tone even when disagreeing. The chatbot provided detailed explanations of Democratic perspectives. This genuine exploration led to dramatic accuracy improvement and an authentic environmental message.

#### R→D Least Improved: R_39bB9XiMhyFaHnj

**Accuracy change**: -2.167 (worsened from 0.751 to 2.918)
**Marketing message**: "Come spend 50% more on less practical appliances since that's what you claim to do and support the business. But I know you won't because just like every Republican said its not financially plausible and your actions only prove the Republicans point."

```{r conversation-rd-least, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_39bB9XiMhyFaHnj.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This conversation was hostile from the start. The participant challenged the chatbot's authenticity, cited external "evidence" (a BBC study) to prove Democrats are hypocrites, and remained adversarial throughout. Rather than exploring Democratic perspectives, the participant used the conversation to confirm negative stereotypes. The chatbot tried to acknowledge concerns and find common ground, but the participant rejected these attempts. This confirmation bias approach led to worsened accuracy and an openly hostile marketing message.

---

### Data Availability

Raw data: [data/raw/qualtrics.parquet](../data/raw/qualtrics.parquet)

Cleaned data: [data/processed/cleaned_data.parquet](../data/processed/cleaned_data.parquet)

Analysis data (minimal, for publication): [data/processed/analysis_data.parquet](../data/processed/analysis_data.parquet)

Conversations: [data/processed/conversations/](../data/processed/conversations/)

### Code Availability

Data cleaning: [src/r/clean_data.R](../src/r/clean_data.R)

Analysis dataset creation: [src/r/create_analysis_data.R](../src/r/create_analysis_data.R)

Conversation download: [src/python/download_conversations.py](../src/python/download_conversations.py)

Extreme cases analysis: [src/r/analyze_extreme_cases.R](../src/r/analyze_extreme_cases.R)

### Pre-Registration

This study was pre-registered prior to data collection. See [docs/pre-registration.md](../docs/pre-registration.md) for complete details including hypotheses, measures, sample size justification, and planned analyses.

### Session Information

```{r session-info}
sessionInfo()
```
