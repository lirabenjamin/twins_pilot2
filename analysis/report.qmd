---
title: "Learning from AI Personas: Can marketers learn about consumers across the political divide by interacting with AI"
author:
  - name: Benjamin Lira
    affiliations:
      - Wharton School, University of Pennsylvania
    email: blira@sas.upenn.edu
  - name: Noah Castelo
    affiliations:
      - Alberta School of Business, University of Alberta
    email: ncastelo@ualberta.ca
  - name: Olivier Toubia
    affiliations:
      - Columbia Business School, Columbia University
    email: ot2107@gsb.columbia.edu
  - name: Stefano Puntoni
    affiliations:
      - Wharton School, University of Pennsylvania
    email: puntoni@wharton.upenn.edu
date: today
abstract: |
  Political polarization poses a fundamental challenge for marketers who must communicate effectively with ideologically diverse consumers. 
  Misperceptions about political outgroups—where partisans hold exaggerated and inaccurate views of those across the political divide—can undermine marketing strategies targeting broad consumer bases. This study tests whether brief interactions with an AI chatbot representing a political outgroup can improve marketers' understanding of that outgroup and reduce affective polarization. 
  Participants interacted with a chatbot prompted to represent their political outgroup and responded to questions about environmental attitudes before and after the interaction. 
  Results demonstrate that chatbot interactions significantly improved both belief accuracy and warmth toward the outgroup. 
  However, these effects were asymmetric: Democrats learning about Republicans showed substantially greater accuracy improvements than Republicans learning about Democrats, though both groups increased warmth at similar rates. 
  Political extremism predicted greater baseline bias and lower warmth but did not moderate the intervention's effectiveness, indicating that both moderate and extreme partisans benefited equally. 
  Bot informativeness predicted better outcomes, while perceived empathy did not. 
  These findings suggest AI-mediated interactions offer a scalable tool for reducing political misperceptions in consumer research, though important questions remain about domain specificity, temporal durability, and how insights from AI interactions transfer to understanding actual outgroup members.
bibliography: ../lit/references.bib
csl: https://www.zotero.org/styles/apa
link-citations: true
format:
  html:
    default: true
    fig-cap-location: bottom
    tbl-cap-location: top
  pdf:
    documentclass: article
    classoption: 
      - 11pt
      - oneside
    keep-tex: true
    fig-cap-location: bottom
    tbl-cap-location: top
    include-in-header: preamble.tex
    metadata:
      author: ""          # prevents Pandoc author injection in some setups
    toc: false
execute:
  echo: false
  cache: true
  warning: false
  message: false
---

## Introduction

```{r setup}
library(tidyverse)
library(arrow)
library(lme4)
library(lmerTest)
library(modelsummary)
library(gt)
library(ggplot2)
library(ggpubr)
library(patchwork)
library(emmeans)
library(tinytable)
library(jsonlite)
library(effectsize)

# switch for data load.
is_running_in_quarto <- function() {
  # Check if a Quarto-specific environment variable is set
  return(Sys.getenv("QUARTO_DOCUMENT_PATH") != "")
}

# Example usage:
if (is_running_in_quarto()) {
  dat <- read_parquet("../data/processed/analysis_data.parquet")
  fig_path <- "../output/figures/"
  tab_path <- "../output/tables/"
} else {
 dat <- read_parquet("./data/processed/analysis_data.parquet")
 fig_path <- "./output/figures/"
 tab_path <- "./output/tables/"
}

# Create output directories if they don't exist
dir.create(fig_path, recursive = TRUE, showWarnings = FALSE)
dir.create(tab_path, recursive = TRUE, showWarnings = FALSE)

# Helper function to save tables
save_table <- function(table_obj, filename, path = tab_path) {
  full_path <- paste0(path, filename)

  # Check if it's a gt table
  if ("gt_tbl" %in% class(table_obj)) {
    gtsave(table_obj, paste0(full_path, ".html"))
    gtsave(table_obj, paste0(full_path, ".png"))
  }
  # Check if it's a tinytable (from modelsummary)
  else if ("tinytable" %in% class(table_obj)) {
    save_tt(table_obj, paste0(full_path, ".html"), overwrite = TRUE)
    save_tt(table_obj, paste0(full_path, ".png"), overwrite = TRUE)
  }

  return(table_obj)
}

# Helper function to format p-values
format_pval <- function(p, digits = 3) {
  threshold <- 10^(-digits)
  if (p < threshold) {
    return(paste0("< .", paste(rep("0", digits - 1), collapse = ""), "1"))
  } else {
    return(sprintf(paste0("%.", digits, "f"), p))
  }
}

# Helper function to format p-values for inline text (with equals sign)
pformat <- function(p, digits = 3) {
  threshold <- 10^(-digits)
  if (p < threshold) {
    return(paste0("< .", paste(rep("0", digits - 1), collapse = ""), "1"))
  } else {
    return(paste0("= ", sprintf(paste0("%.", digits, "f"), p)))
  }
}

# Helper function to format numbers with specified decimal places
numformat <- function(x, digits = 2) {
  sprintf(paste0("%.", digits, "f"), x)
}

# Load cleaned data


# Define consistent color scheme: Democrats = blue, Republicans = red
party_colors <- c("Democrat" = "#0015BC", "Republican" = "#E81B23")
party_colors_dark <- c("Democrat" = "#000a5e", "Republican" = "#750b0f")

# Define learner party colors (D→R = Democrat learning about Republicans, R→D = Republican learning about Democrats)
learner_party_colors <- c("D→R" = "#0015BC", "R→D" = "#E81B23")

# Define common coefficient mapping for all modelsummary tables
coef_map_common <- c(
  "timepost" = "Time (Post)",
  "learner_partyR→D" = "Learner Party (R→D)",
  "political_extremism" = "Political Extremism",
  "timepost:learner_partyR→D" = "Time × Learner Party",
  "timepost:political_extremism" = "Time × Extremism",
  "learner_partyR→D:political_extremism" = "Learner Party × Extremism",
  "timepost:learner_partyR→D:political_extremism" = "Time × Learner Party × Extremism",
  "accuracy_pre" = "Baseline Accuracy",
  "warmth_outgroup_pre" = "Baseline Warmth",
  "confidence_outgroup_pre" = "Baseline Confidence",
  "info" = "Bot Informativeness",
  "empathy" = "Bot Empathy",
  "user_turn_count" = "Turn Count",
  "log_word_count" = "Log(Word Count)",
  "SD (Intercept ResponseID)" = "SD(Intercept)"
)

# Calculate sample composition
n_dr <- dat %>% filter(learner_party == "D→R") %>% nrow()
n_rd <- dat %>% filter(learner_party == "R→D") %>% nrow()
```


Political polarization undermines marketers' ability to understand and serve ideologically diverse consumers [@hsieh_partisan_2024].
Partisans hold exaggerated views of political outgroups [@iyengar_origins_2019; @yudkin_perception_2019], misperceptions that can lead to ineffective marketing strategies when firms target broad consumer bases.

Intergroup contact reduces prejudice and corrects misperceptions [@allport1954nature; @pettigrew_meta-analytic_2006], but real-world contact is logistically costly, socially awkward, and increasingly rare across party lines.
Recent advances in AI enable "synthetic contact"—conversations with AI chatbots representing outgroup members—that may replicate these benefits at scale [@hermann_reducing_2025].

Whether AI-mediated contact can genuinely reduce political misperceptions is unclear.
On one hand, chatbot interactions might provide corrective information about outgroup attitudes while fostering empathic understanding through open dialogue.
On the other hand, participants may dismiss AI-generated perspectives as inauthentic or may selectively attend to information that confirms existing biases.
Even if participants update their beliefs, they may lack metacognitive awareness of this learning—becoming more accurate without becoming more confident, or vice versa.

We test whether brief interactions with an AI chatbot representing a political outgroup improve belief accuracy and reduce affective polarization.
We examine both informational mechanisms (does the chatbot provide useful information?) and empathic mechanisms (does the chatbot foster emotional connection?), and assess whether participants gain metacognitive insight into their improved understanding.
We recruited `r nrow(dat)` partisan participants (prescreened to include participants who identified as democrats or republicans; `r n_dr` Democrats, `r n_rd` Republicans).
Participants reported their own environmental attitudes, estimated the environmental attitudes of the average Democrat and Republican, and rated their warmth toward each political group.
They then conversed for 10 minutes with a chatbot prompted to represent their political outgroup before re-rating outgroup warmth and re-estimating outgroup environmental attitudes.
We hypothesized that participants would report more accurate beliefs about outgroup environmental attitudes and warmer feelings toward the political outgroup after the interaction.



## Results

### Partisans misperceive each other

```{r baseline-perception-stats}
# Compare Democrats' own attitudes vs Republicans' own attitudes
own_attitudes_test <- dat %>%
  rstatix::t_test(green_own ~ participant_party)
own_attitudes_d <- dat %>%
  rstatix::cohens_d(green_own ~ participant_party) %>%
  pull(effsize) %>%
  numformat()
own_attitudes_p <- own_attitudes_test$p

# Calculate means for own attitudes by party
dem_own_mean <- dat %>% filter(participant_party == "Democrat") %>% pull(green_own) %>% mean()
rep_own_mean <- dat %>% filter(participant_party == "Republican") %>% pull(green_own) %>% mean()

# Ingroup perception accuracy - Democrats evaluating Democrats
dem_ingroup_actual <- dat %>% filter(participant_party == "Democrat") %>% pull(green_own) %>% mean()
dem_ingroup_perceived <- dat %>% filter(participant_party == "Democrat") %>% pull(green_ingroup) %>% mean()
dem_ingroup_test <- t.test(
  dat %>% filter(participant_party == "Democrat") %>% pull(green_ingroup),
  mu = dem_ingroup_actual
)
dem_ingroup_d <- (dem_ingroup_perceived - dem_ingroup_actual) / sd(dat %>% filter(participant_party == "Democrat") %>% pull(green_ingroup))
dem_ingroup_p <- dem_ingroup_test$p.value

# Ingroup perception accuracy - Republicans evaluating Republicans
rep_ingroup_actual <- dat %>% filter(participant_party == "Republican") %>% pull(green_own) %>% mean()
rep_ingroup_perceived <- dat %>% filter(participant_party == "Republican") %>% pull(green_ingroup) %>% mean()
rep_ingroup_test <- t.test(
  dat %>% filter(participant_party == "Republican") %>% pull(green_ingroup),
  mu = rep_ingroup_actual
)
rep_ingroup_d <- (rep_ingroup_perceived - rep_ingroup_actual) / sd(dat %>% filter(participant_party == "Republican") %>% pull(green_ingroup))
rep_ingroup_p <- rep_ingroup_test$p.value
```

Participants systematically misperceived the environmental attitudes of their political opponents.
Most participants held strong partisan identities, with the majority in the top and bottom quartiles of the political spectrum (@fig-political-orientation).
Democrats rated themselves higher in environmental concern (*M* = `r sprintf("%.2f", dem_own_mean)`) relative to Republicans (*M* = `r sprintf("%.2f", rep_own_mean)`), *d* = `r own_attitudes_d`, *p* `r pformat(own_attitudes_p)`.
While participants accurately perceived their ingroups (Democrats: *M*~actual~ = `r sprintf("%.2f", dem_ingroup_actual)`, *M*~perceived~ = `r sprintf("%.2f", dem_ingroup_perceived)`, *p* `r pformat(dem_ingroup_p)`; Republicans: *M*~actual~ = `r sprintf("%.2f", rep_ingroup_actual)`, *M*~perceived~ = `r sprintf("%.2f", rep_ingroup_perceived)`, *p* `r pformat(rep_ingroup_p)`), they systematically underestimated how environmentally friendly their counterparts actually were.
Democrats misperceived Republicans far more than Republicans misperceived Democrats.

```{r prepare-long-data}
# Prepare long format datasets
accuracy_long <- dat %>%
  select(ResponseId, learner_party, accuracy_pre, accuracy_post) %>%
  pivot_longer(cols = c(accuracy_pre, accuracy_post), names_to = "time",
               names_pattern = "accuracy_(pre|post)", values_to = "error") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

warmth_long <- dat %>%
  select(ResponseId, learner_party, warmth_outgroup_pre, warmth_outgroup_post) %>%
  pivot_longer(cols = c(warmth_outgroup_pre, warmth_outgroup_post), names_to = "time",
               names_pattern = "warmth_outgroup_(pre|post)", values_to = "warmth") %>%
  mutate(time = factor(time, levels = c("pre", "post")))
```

```{r}
#| label: fig-perception-wedge
#| fig-cap: "The perception wedge: Partisans overestimate differences in environmental attitudes. Left panel shows Republicans' own attitudes (solid) versus how Democrats perceive Republicans (dashed). Middle panel shows Democrats' own attitudes (solid) versus how Republicans perceive Democrats (dashed). Right panel shows the distribution of baseline bias (absolute error), with Democrats showing greater bias about Republicans than vice versa. The gap between perceived and actual attitudes demonstrates systematic misperception across the political divide."
#| fig-width: 12
#| fig-height: 3.5


# Panel 1: Republicans - actual vs. perceived by Democrats
# Republicans' own attitudes
rep_actual <- dat %>%
  filter(participant_party == "Republican") %>%
  pull(green_own)

# How Democrats perceive Republicans (Democrats' outgroup estimates)
rep_perceived <- dat %>%
  filter(participant_party == "Democrat") %>%
  pull(green_outgroup_pre)

rep_data <- tibble(
  value = c(rep_actual, rep_perceived),
  measure = rep(c("Actual",
                  "Perceived by Outgroup"),
                c(length(rep_actual), length(rep_perceived)))
)

p1 <- ggplot(rep_data, aes(x = value, linetype = measure)) +
  geom_density(color = party_colors["Republican"], fill = party_colors["Republican"],
               alpha = 0.2, linewidth = 1) +
  scale_linetype_manual(values = c("solid", "dashed"), name = NULL) +
  labs(x = "Green Attitudes (1-5)", y = "Density",
       title = "Republicans (real vs. perceived)") +
  theme_minimal() +
  annotate(geom = 'text', x = c(4, 2), y = .4, label = c("Actual", "Outgroup Perceptions"))+
  theme(legend.position = "bottom",
        legend.text = element_text(size = 8),
        plot.title = element_text(hjust = 0.5, face = "bold"))


# Panel 2: Democrats - actual vs. perceived by Republicans
# Democrats' own attitudes
dem_actual <- dat %>%
  filter(participant_party == "Democrat") %>%
  pull(green_own)

# How Republicans perceive Democrats (Republicans' outgroup estimates)
dem_perceived <- dat %>%
  filter(participant_party == "Republican") %>%
  pull(green_outgroup_pre)

dem_data <- tibble(
  value = c(dem_actual, dem_perceived),
  measure = rep(c("Actual",
                  "Perceived by outgroup"),
                c(length(dem_actual), length(dem_perceived)))
)

p2 <- ggplot(dem_data, aes(x = value, linetype = measure)) +
  geom_density(color = party_colors["Democrat"], fill = party_colors["Democrat"],
               alpha = 0.2, linewidth = 1) +
                 annotate(geom = 'text', x = c(4, 4), y = c(.5, .4), label = c("Actual", "Outgroup Perceptions"))+

  scale_linetype_manual(values = c("solid", "dashed"), name = NULL) +
  labs(x = "Green Attitudes (1-5)", y = "Density",
       title = "Democrats (real vs. perceived)") +
  theme_minimal() +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 8),
        plot.title = element_text(hjust = 0.5, face = "bold"))

# Panel 3: Distribution of bias by party
p3 <- dat %>%
  ggplot(aes(x = accuracy_pre, fill = participant_party)) +
  geom_density(color = NA, alpha = 0.6) +
  scale_fill_manual(
    values = party_colors,
    name = NULL
  ) +
  labs(
    x = "Baseline Accuracy",
    y = "Density",
    title = "Magnitude of Bias",
    subtitle = "Democrats are less accurate"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 8),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "bold", color = 'gray40')
  )

# Combine panels
p_combined <- p1 + p2 + p3 + plot_layout(guides = "collect") &
  theme(legend.position = "bottom")

ggsave(paste0(fig_path, "fig_perception_wedge.png"), p_combined,
       width = 12, height = 3.5, dpi = 300)
p_combined
```



```{r}
#| label: fig-political-orientation
#| fig-cap: "Distribution of political orientation among participants. Values below 50 indicate Democratic leaning, values 50 and above indicate Republican leaning."
#| fig-height: 3
#| fig-width: 4
p <- dat %>%
  ggplot(aes(politicalorientation_1)) +
  geom_histogram(aes(fill = after_stat(x))) +
  labs(x = 'Political Orientation', y = "Count")+
  theme_minimal()+
  theme(legend.position = "none")+
  scale_fill_gradient2(low = party_colors["Democrat"],
                       mid = "gray40",
                       high = party_colors["Republican"],
                       midpoint = 50)  # Adjust midpoint to your scale center
ggsave(paste0(fig_path, "fig_political_orientation.png"), p, width = 4, height = 3, dpi = 300)
p
```


```{r}
#| label: fig-green-attitudes-perception
#| fig-cap: "Perceptions of environmental attitudes across the political divide. Four panels show each evaluator-target combination. Top row: Democrats evaluating themselves (ingroup) and Republicans (outgroup). Bottom row: Republicans evaluating themselves (ingroup) and Democrats (outgroup). Actual attitudes shown as reference; perceived attitudes shown before and after chatbot interaction."
#| fig-width: 10
#| fig-height: 6

# Panel 1: Democrats evaluating Democrats (themselves - ingroup)
dem_eval_dem_actual <- dat %>% filter(participant_party == "Democrat") %>% pull(green_own)
dem_eval_dem_perceived <- dat %>% filter(participant_party == "Democrat") %>% pull(green_ingroup)

dem_eval_dem <- tibble(
  value = c(dem_eval_dem_actual, dem_eval_dem_perceived),
  measure_type = rep(c("Actual", "Perceived"),
                     c(length(dem_eval_dem_actual), length(dem_eval_dem_perceived))),
  panel = "Democrats → Democrats"
)

# Panel 2: Democrats evaluating Republicans (outgroup) - no actual bar
dem_eval_rep_pre <- dat %>% filter(participant_party == "Democrat") %>% pull(green_outgroup_pre)
dem_eval_rep_post <- dat %>% filter(participant_party == "Democrat") %>% pull(green_outgroup_post)

dem_eval_rep <- tibble(
  value = c(dem_eval_rep_pre, dem_eval_rep_post),
  measure_type = rep(c("Perceived (Pre)", "Perceived (Post)"),
                     c(length(dem_eval_rep_pre), length(dem_eval_rep_post))),
  panel = "Democrats → Republicans"
)

# Panel 3: Republicans evaluating Republicans (themselves - ingroup)
rep_eval_rep_actual <- dat %>% filter(participant_party == "Republican") %>% pull(green_own)
rep_eval_rep_perceived <- dat %>% filter(participant_party == "Republican") %>% pull(green_ingroup)

rep_eval_rep <- tibble(
  value = c(rep_eval_rep_actual, rep_eval_rep_perceived),
  measure_type = rep(c("Actual", "Perceived"),
                     c(length(rep_eval_rep_actual), length(rep_eval_rep_perceived))),
  panel = "Republicans → Republicans"
)

# Panel 4: Republicans evaluating Democrats (outgroup) - no actual bar
rep_eval_dem_pre <- dat %>% filter(participant_party == "Republican") %>% pull(green_outgroup_pre)
rep_eval_dem_post <- dat %>% filter(participant_party == "Republican") %>% pull(green_outgroup_post)

rep_eval_dem <- tibble(
  value = c(rep_eval_dem_pre, rep_eval_dem_post),
  measure_type = rep(c("Perceived (Pre)", "Perceived (Post)"),
                     c(length(rep_eval_dem_pre), length(rep_eval_dem_post))),
  panel = "Republicans → Democrats"
)

# Create reference lines for actual values in outgroup panels
rep_actual_mean <- dat %>%
  filter(participant_party == "Republican") %>%
  summarise(mean_se(green_own))

dem_actual_mean <- dat %>%
  filter(participant_party == "Democrat") %>%
  summarise(mean_se(green_own))

actual_refs <- tibble(
  panel = factor(c("Democrats → Republicans", "Republicans → Democrats"),
                 levels = c("Democrats → Democrats", "Democrats → Republicans",
                            "Republicans → Republicans", "Republicans → Democrats")),
  y = c(rep_actual_mean$y, dem_actual_mean$y),
  ymin = c(rep_actual_mean$ymin, dem_actual_mean$ymin),
  ymax = c(rep_actual_mean$ymax, dem_actual_mean$ymax),
  label_x = 1.5,
  label_y = c(rep_actual_mean$y + 0.15, dem_actual_mean$y + 0.15),
  label = "Actual",
  color = c(party_colors["Republican"], party_colors["Democrat"])
)

# Combine all panels
all_perception_data <- bind_rows(dem_eval_dem, dem_eval_rep, rep_eval_rep, rep_eval_dem) %>%
  mutate(
    panel = factor(panel, levels = c("Democrats → Democrats", "Democrats → Republicans",
                                      "Republicans → Republicans", "Republicans → Democrats")),
    measure_type = factor(measure_type, levels = c("Actual", "Perceived", "Perceived (Pre)", "Perceived (Post)")),
    evaluator = case_when(
      grepl("^Democrats", panel) ~ "Democrat",
      grepl("^Republicans", panel) ~ "Republican"
    ),
    is_post = measure_type == "Perceived (Post)"
  )

# Create color scheme
all_perception_data <- all_perception_data %>%
  mutate(
    fill_color = case_when(
      is_post & evaluator == "Democrat" ~ party_colors_dark["Democrat"],
      is_post & evaluator == "Republican" ~ party_colors_dark["Republican"],
      evaluator == "Democrat" ~ party_colors["Democrat"],
      evaluator == "Republican" ~ party_colors["Republican"]
    )
  )

p <- ggplot(all_perception_data, aes(x = measure_type, y = value, fill = fill_color)) +
  facet_wrap(~panel, ncol = 4, scales = 'free_x') +
  stat_summary(fun = mean, geom = "bar") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2) +
  geom_rect(data = actual_refs, aes(xmin = -Inf, xmax = Inf, ymin = ymin, ymax = ymax),
            alpha = 0.3, fill = "gray40", inherit.aes = FALSE) +
  geom_hline(data = actual_refs, aes(yintercept = y, color = color),
             linewidth = 1, linetype = "solid") +
  geom_text(data = actual_refs, aes(x = label_x, y = label_y, label = label, color = color),
            size = 3, fontface = "bold", inherit.aes = FALSE) +
  scale_fill_identity() +
  scale_color_identity() +
  labs(
    x = NULL,
    y = "Green Attitude (1-5 scale)"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

ggsave(paste0(fig_path, "fig_green_attitudes_perception.png"), p, width = 10, height = 6, dpi = 300)
p
```

```{r bias}
# Calculate baseline bias comparison
bias_test <- dat %>%
  select(learner_party, accuracy_pre) %>%
  rstatix::t_test(accuracy_pre~learner_party)

p <- bias_test %>% pull(p)

d <- dat %>%
  select(learner_party, accuracy_pre) %>%
  rstatix::cohens_d(accuracy_pre~learner_party) %>% pull(effsize) %>% numformat()

# Calculate means and SDs by group
bias_descriptives <- dat %>%
  group_by(learner_party) %>%
  summarise(
    mean_bias = mean(accuracy_pre, na.rm = TRUE),
    sd_bias = sd(accuracy_pre, na.rm = TRUE)
  )

mean_dr <- bias_descriptives %>% filter(learner_party == "D→R") %>% pull(mean_bias)
sd_dr <- bias_descriptives %>% filter(learner_party == "D→R") %>% pull(sd_bias)
mean_rd <- bias_descriptives %>% filter(learner_party == "R→D") %>% pull(mean_bias)
sd_rd <- bias_descriptives %>% filter(learner_party == "R→D") %>% pull(sd_bias)
```


We quantified this bias by calculating each participant's absolute error from the actual mean and reverse-scoring it so that higher values indicate greater accuracy. Democrats showed significantly greater bias about Republicans (*M* = `r sprintf("%.2f", mean_dr)`, *SD* = `r sprintf("%.2f", sd_dr)`) than Republicans showed about Democrats (*M* = `r sprintf("%.2f", mean_rd)`, *SD* = `r sprintf("%.2f", sd_rd)`), *d* = `r d`, *p* `r pformat(p)` (@fig-bias-distribution).



```{r}
#| label: fig-bias-distribution
#| fig-cap: "Distribution of baseline accuracy in estimating outgroup environmental attitudes. Left panel shows overall distribution; right panel shows distribution by political party. Democrats were more biased about Republicans than Republicans were about Democrats."
#| fig-subcap:
#|   - "Overall distribution of baseline accuracy"
#|   - "Baseline accuracy by political party"
#| layout-ncol: 2
#| fig-width: 4
#| fig-height: 3

# Overall distribution
p1 <- dat %>%
  ggplot(aes(x = accuracy_pre)) +
  geom_density(color = NA, fill = "gray30", alpha = 0.6) +
  labs(
    x = "Baseline Accuracy (higher = more accurate)",
    y = "Density"
  ) +
  theme_minimal()

# By participant party
p2 <- dat %>%
  ggplot(aes(x = accuracy_pre, fill = participant_party)) +
  geom_density(color = NA, alpha = 0.6) +
  scale_fill_manual(
    values = c("Democrat" = "steelblue", "Republican" = "firebrick"),
    name = NULL
  ) +
  labs(
    x = "Baseline Accuracy (higher = more accurate)",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    legend.position = c(1-0.85, 0.85)
  )

ggsave(paste0(fig_path, "fig_bias_distribution_overall.png"), p1, width = 4, height = 3, dpi = 300)
ggsave(paste0(fig_path, "fig_bias_distribution_by_party.png"), p2, width = 4, height = 3, dpi = 300)
p1
p2
```

```{r baseline-accuracy-warmth-correlation}
# Calculate correlations between baseline accuracy and warmth by party
cor_dem <- dat %>%
  filter(participant_party == "Democrat") %>%
  with(cor.test(accuracy_pre, warmth_outgroup_pre))

cor_rep <- dat %>%
  filter(participant_party == "Republican") %>%
  with(cor.test(accuracy_pre, warmth_outgroup_pre))

cor_dem_r <- cor_dem$estimate
cor_dem_p <- cor_dem$p.value
cor_rep_r <- cor_rep$estimate
cor_rep_p <- cor_rep$p.value
```

Participants who at baseline felt warmer towards the outgroup were also more accurate in their perceptions. This relationship held for both Democrats (*r* = `r sprintf("%.2f", cor_dem_r)`, *p* `r pformat(cor_dem_p)`) and Republicans (*r* = `r sprintf("%.2f", cor_rep_r)`, *p* `r pformat(cor_rep_p)`).

```{r}
#| label: fig-accuracy-warmth-baseline
#| fig-cap: "Relationship between baseline accuracy and warmth toward political outgroup. Points represent individual participants, lines show linear fits by party. Correlation statistics displayed on plot."
#| fig-width: 6
#| fig-height: 4

p <- dat %>%
  ggplot(aes(x = accuracy_pre, y = warmth_outgroup_pre, color = participant_party)) +
  geom_point(alpha = 0.6, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = party_colors, name = "Political Party") +
  labs(
    x = "Baseline Accuracy (higher = more accurate)",
    y = "Baseline Warmth (0-100 scale)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_baseline_accuracy_warmth.png"), p, width = 6, height = 4, dpi = 300)
p
```

### Do people update their beliefs after interacting with the chatbot?

```{r q1-data-prep}
# Calculate summary statistics for plots
accuracy_summary <- accuracy_long %>%
  group_by(learner_party, time) %>%
  summarise(mean_error = mean(error, na.rm = TRUE),
            se_error = sd(error, na.rm = TRUE) / sqrt(n()),
            .groups = "drop")

warmth_summary <- warmth_long %>%
  group_by(learner_party, time) %>%
  summarise(mean_warmth = mean(warmth, na.rm = TRUE),
            se_warmth = sd(warmth, na.rm = TRUE) / sqrt(n()),
            .groups = "drop")
```



```{r q1-models}
# Estimate mixed-effects models
accuracy_model0 <- lmer(error ~ time  + (1 | ResponseId), data = accuracy_long)
warmth_model0 <- lmer(warmth ~ time  + (1 | ResponseId), data = warmth_long)

accuracy_model <- lmer(error ~ time * learner_party + (1 | ResponseId), data = accuracy_long)
warmth_model <- lmer(warmth ~ time * learner_party + (1 | ResponseId), data = warmth_long)

# Extract coefficients for inline reporting
accuracy_time_coef <- fixef(accuracy_model0)["timepost"]
warmth_time_coef <- fixef(warmth_model0)["timepost"]

# Calculate Cohen's d for main effects
accuracy_d <- -cohens_d(error ~ time, data = accuracy_long, pooled_sd = FALSE)$Cohens_d
warmth_d <- cohens_d(warmth ~ time, data = warmth_long, pooled_sd = FALSE)$Cohens_d

```

We examined whether participants changed their beliefs about the outgroup's environmental attitudes from pre- to post-interaction, analyzing both accuracy and warmth.
As shown in the table below, participants became significantly more accurate (improving by `r sprintf("%.2f", accuracy_time_coef)` points on the 5-point scale, *d* = `r sprintf("%.2f", accuracy_d)`) and warmer toward the outgroup (increasing by `r sprintf("%.1f", warmth_time_coef)` degrees on the 100-point thermometer, *d* = `r sprintf("%.2f", warmth_d)`).

```{r q1-plots}
#| label: fig-accuracy-warmth-change
#| fig-cap: "Changes in belief accuracy and outgroup warmth from pre- to post-interaction. Left panel shows participants became more accurate in estimating outgroup environmental attitudes. Right panel shows participants reported increased warmth toward the political outgroup on a 0-100 feeling thermometer. Error bars represent standard errors. Both Democrats learning about Republicans (D→R) and Republicans learning about Democrats (R→D) showed significant improvements."
#| fig-width: 10
#| fig-height: 4

# Combine plots side by side using patchwork
p_accuracy <- ggplot(accuracy_summary, aes(x = time, y = mean_error, color = learner_party, group = learner_party)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_error - se_error, ymax = mean_error + se_error), width = 0.1) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(x = "Time", y = "Mean Accuracy") +
  theme_minimal() +
  theme(legend.position = "bottom")

p_warmth <- ggplot(warmth_summary, aes(x = time, y = mean_warmth, color = learner_party, group = learner_party)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_warmth - se_warmth, ymax = mean_warmth + se_warmth), width = 0.1) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(x = "Time", y = "Mean Warmth") +
  theme_minimal() +
  theme(legend.position = "bottom")

p_combined <- p_accuracy + p_warmth + plot_layout(guides = "collect") & theme(legend.position = "bottom")
ggsave(paste0(fig_path, "fig_q1_accuracy_warmth.png"), p_combined, width = 10, height = 4, dpi = 300)
p_combined
```

```{r}
#| label: tbl-main-effects
#| tbl-cap: "Mixed-effects models predicting belief accuracy and outgroup warmth from chatbot interaction timing. Models include random intercepts by participant. Time coefficient represents the main effect of pre- to post-interaction change. Both outcomes showed significant improvement following the chatbot interaction."
# Display models side by side
tbl <- modelsummary(
  list(
    "Accuracy" = accuracy_model0,
    "Warmth" = warmth_model0),
  stars = TRUE,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Accuracy is negative absolute error (higher = more accurate). * p<0.05, ** p<0.01, *** p<0.001"
)
save_table(tbl, "tab_q1_main_effects")
```

```{r accuracy-warmth-change-stats}
# Create change scores for correlation analysis
dat_with_changes <- dat %>%
  mutate(
    accuracy_change = accuracy_post - accuracy_pre,  # Negative = improvement
    warmth_change = warmth_outgroup_post - warmth_outgroup_pre
  )

# Calculate correlations between accuracy change and warmth change by party
cor_change_dr <- dat_with_changes %>%
  filter(learner_party == "D→R") %>%
  with(cor.test(accuracy_change, warmth_change))

cor_change_rd <- dat_with_changes %>%
  filter(learner_party == "R→D") %>%
  with(cor.test(accuracy_change, warmth_change))

cor_change_dr_r <- cor_change_dr$estimate
cor_change_dr_p <- cor_change_dr$p.value
cor_change_rd_r <- cor_change_rd$estimate
cor_change_rd_p <- cor_change_rd$p.value
```

Democrats who became more accurate towards Republicans also became warmer towards them (*r* = `r sprintf("%.2f", cor_change_dr_r)`, *p* `r pformat(cor_change_dr_p)`), but this relationship did not hold for Republicans learning about Democrats (*r* = `r sprintf("%.2f", cor_change_rd_r)`, *p* `r pformat(cor_change_rd_p)`).

```{r accuracy-warmth-change-correlation}
#| label: fig-accuracy-warmth-correlation
#| fig-cap: "Correlation between changes in belief accuracy and changes in outgroup warmth. Points represent individual participants, with separate regression lines for Democrats learning about Republicans (D→R) and Republicans learning about Democrats (R→D). Negative values on x-axis indicate improved accuracy. Correlation statistics are displayed separately by learner party."
#| fig-width: 6
#| fig-height: 4

p <- dat_with_changes %>%
  ggplot(aes(x = accuracy_change, y = warmth_change, color = learner_party)) +
  geom_point(alpha = 0.6, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Accuracy Change (Negative = Improvement)",
    y = "Warmth Change (Positive = Increase)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_accuracy_warmth_change.png"), p, width = 6, height = 4, dpi = 300)
p
```

### Is belief updating symmetric across political groups?

```{r simple-effects-party}
# Get simple effects
# Note: reverse = TRUE ensures contrasts are computed as "post - pre" to match model coefficients
accuracy_contrasts <- emmeans(accuracy_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

warmth_contrasts <- emmeans(warmth_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Extract values for inline reporting
acc_dr_est <- abs(accuracy_contrasts$estimate[1])  # D→R effect (negative = improvement)
acc_dr_p <- accuracy_contrasts$p.value[1]
acc_rd_est <- abs(accuracy_contrasts$estimate[2])  # R→D effect
acc_rd_p <- accuracy_contrasts$p.value[2]

warmth_dr_est <- warmth_contrasts$estimate[1]  # D→R effect
warmth_dr_p <- warmth_contrasts$p.value[1]
warmth_rd_est <- warmth_contrasts$estimate[2]  # R→D effect
warmth_rd_p <- warmth_contrasts$p.value[2]

# Calculate Cohen's d for simple effects by learner party
acc_dr_d <- -cohens_d(error ~ time, data = accuracy_long %>% filter(learner_party == "D→R"), pooled_sd = FALSE)$Cohens_d
acc_rd_d <- -cohens_d(error ~ time, data = accuracy_long %>% filter(learner_party == "R→D"), pooled_sd = FALSE)$Cohens_d

warmth_dr_d <- cohens_d(warmth ~ time, data = warmth_long %>% filter(learner_party == "D→R"), pooled_sd = FALSE)$Cohens_d
warmth_rd_d <- cohens_d(warmth ~ time, data = warmth_long %>% filter(learner_party == "R→D"), pooled_sd = FALSE)$Cohens_d

# Get interaction term from model
acc_interaction_coef <- fixef(accuracy_model)["timepost:learner_partyR→D"]
acc_interaction_p <- summary(accuracy_model)$coefficients["timepost:learner_partyR→D", "Pr(>|t|)"]

warmth_interaction_coef <- fixef(warmth_model)["timepost:learner_partyR→D"]
warmth_interaction_p <- summary(warmth_model)$coefficients["timepost:learner_partyR→D", "Pr(>|t|)"]

# Format for modelsummary
format_emmeans <- function(contrasts) {
  contrasts %>%
    mutate(
      stars = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01 ~ "**",
        p.value < 0.05 ~ "*",
        TRUE ~ ""
      ),
      formatted = sprintf("%.3f%s", estimate, stars),
      se_formatted = sprintf("(%.3f)", std.error)
    )
}

acc_formatted <- format_emmeans(accuracy_contrasts)
warmth_formatted <- format_emmeans(warmth_contrasts)

rows = tibble(
    term = c("Time effect (Democrat)", "", "Time effect (Republican)", ""),
    `Accuracy` = c(acc_formatted$formatted[1], acc_formatted$se_formatted[1],
                   acc_formatted$formatted[2], acc_formatted$se_formatted[2]),
    `Warmth` = c(warmth_formatted$formatted[1], warmth_formatted$se_formatted[1],
                 warmth_formatted$formatted[2], warmth_formatted$se_formatted[2])
  )
attr(rows, "position") <- c(1, 2, 3, 4) # Inserts at rows 1 and 2

```

While both groups became more accurate, it was Democrats learning about Republicans who showed greater improvement (*b* = `r sprintf("%.2f", acc_dr_est)`, *d* = `r sprintf("%.2f", acc_dr_d)`, *p* `r if(acc_dr_p < 0.001) "< .001" else paste("=", sprintf("%.3f", acc_dr_p))`).
Republicans also became less biased about Democrats, though to a lesser extent (*b* = `r sprintf("%.2f", acc_rd_est)`, *d* = `r sprintf("%.2f", acc_rd_d)`, *p* `r if(acc_rd_p < 0.001) "< .001" else paste("=", sprintf("%.3f", acc_rd_p))`).
The interaction term was `r if(acc_interaction_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.2f", acc_interaction_coef)`, *p* `r if(acc_interaction_p < 0.001) "< .001" else paste("=", sprintf("%.3f", acc_interaction_p))`).

Both groups became warmer toward the outgroup at similar rates.
Democrats learning about Republicans increased warmth by `r sprintf("%.1f", warmth_dr_est)` degrees (*d* = `r sprintf("%.2f", warmth_dr_d)`, *p* `r if(warmth_dr_p < 0.001) "< .001" else paste("=", sprintf("%.3f", warmth_dr_p))`), while Republicans learning about Democrats increased by `r sprintf("%.1f", warmth_rd_est)` degrees (*d* = `r sprintf("%.2f", warmth_rd_d)`, *p* `r if(warmth_rd_p < 0.001) "< .001" else paste("=", sprintf("%.3f", warmth_rd_p))`).
The interaction was `r if(warmth_interaction_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.2f", warmth_interaction_coef)`, *p* `r if(warmth_interaction_p < 0.001) "< .001" else paste("=", sprintf("%.3f", warmth_interaction_p))`).

```{r}
#| label: tbl-symmetry
#| tbl-cap: "Tests of asymmetry in belief updating across political groups. Upper panel shows simple effects of time separately for Democrats learning about Republicans (D→R) and Republicans learning about Democrats (R→D). Lower panel shows full mixed-effects regression results including the Time × Learner Party interaction term. Democrats showed significantly larger accuracy improvements than Republicans, while warmth increases were symmetric across groups."
# Create the table with add_rows
tbl <- modelsummary(
  list("Accuracy" = accuracy_model, "Warmth" = warmth_model),
  stars = TRUE,
  add_rows = rows,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Accuracy is negative absolute error (higher = more accurate). * p<0.05, ** p<0.01, *** p<0.001"
) %>%
 strip_tt(line = TRUE) |>
    group_tt(i = list("Effects by Political Affiliation" = 1, "Full Regression Results" = 5)) |>
      style_tt(i = c(1, 6), bold = TRUE) |>
    style_tt(i = c(14, 16), line = "b", line_color = "lightgray")
save_table(tbl, "tab_q2_interaction")
```

### Does political extremity predict how much people update?

```{r extremism-models}
# Add extremism to long data
accuracy_long_ext <- accuracy_long %>%
  left_join(dat %>% select(ResponseId, political_extremism), by = "ResponseId")

warmth_long_ext <- warmth_long %>%
  left_join(dat %>% select(ResponseId, political_extremism), by = "ResponseId")

# First, test extremism predicting baseline outcomes
baseline_accuracy_extremism <- lm(accuracy_pre ~ political_extremism, data = dat)
baseline_warmth_extremism <- lm(warmth_outgroup_pre ~ political_extremism, data = dat)

# Extract coefficients for inline reporting
extremism_acc_baseline_b <- coef(baseline_accuracy_extremism)["political_extremism"]
extremism_acc_baseline_p <- summary(baseline_accuracy_extremism)$coefficients["political_extremism", "Pr(>|t|)"]

extremism_warmth_baseline_b <- coef(baseline_warmth_extremism)["political_extremism"]
extremism_warmth_baseline_p <- summary(baseline_warmth_extremism)$coefficients["political_extremism", "Pr(>|t|)"]

# Calculate correlation-based effect sizes for baseline extremism effects
extremism_acc_baseline_r <- cor(dat$political_extremism, dat$accuracy_pre, use = "complete.obs")
extremism_warmth_baseline_r <- cor(dat$political_extremism, dat$warmth_outgroup_pre, use = "complete.obs")

# Models with extremism interaction
accuracy_extremism_model0 <- lmer(
  error ~ time * political_extremism + learner_party + (1 | ResponseId),
  data = accuracy_long_ext
)

warmth_extremism_model0 <- lmer(
  warmth ~ time * political_extremism + learner_party + (1 | ResponseId),
  data = warmth_long_ext
)


# Models with extremism interaction
accuracy_extremism_model <- lmer(
  error ~ time * learner_party * political_extremism + (1 | ResponseId),
  data = accuracy_long_ext
)

warmth_extremism_model <- lmer(
  warmth ~ time * learner_party * political_extremism + (1 | ResponseId),
  data = warmth_long_ext
)

# Extract three-way interaction terms for inline reporting
extremism_3way_acc_coef <- fixef(accuracy_extremism_model)["timepost:learner_partyR→D:political_extremism"]
extremism_3way_acc_p <- summary(accuracy_extremism_model)$coefficients["timepost:learner_partyR→D:political_extremism", "Pr(>|t|)"]

extremism_3way_warmth_coef <- fixef(warmth_extremism_model)["timepost:learner_partyR→D:political_extremism"]
extremism_3way_warmth_p <- summary(warmth_extremism_model)$coefficients["timepost:learner_partyR→D:political_extremism", "Pr(>|t|)"]


# Get simple effects at different extremism levels (low = -1 SD, high = +1 SD)
extremism_mean <- mean(accuracy_long_ext$political_extremism, na.rm = TRUE)
extremism_sd <- sd(accuracy_long_ext$political_extremism, na.rm = TRUE)

low_extremism = extremism_mean - extremism_sd
high_extremism = extremism_mean + extremism_sd

accuracy_extremism_contrasts <- emmeans(accuracy_extremism_model,
  ~ time | political_extremism,
  at = list(political_extremism = c(low_extremism, high_extremism))) %>%
  pairs(reverse = TRUE) %>%
  tidy()

warmth_extremism_contrasts <- emmeans(warmth_extremism_model, ~ time | political_extremism,
at = list(political_extremism = c(low_extremism, high_extremism))
) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Format simple effects
acc_ext_formatted <- accuracy_extremism_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

warmth_ext_formatted <- warmth_extremism_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

rows_extremism <- tibble(
  term = c("Time effect (Low Extremism)", "", "Time effect (High Extremism)", ""),
  `Accuracy` = c(acc_ext_formatted$formatted[1], acc_ext_formatted$se_formatted[1],
                 acc_ext_formatted$formatted[2], acc_ext_formatted$se_formatted[2]),
  `Warmth` = c(warmth_ext_formatted$formatted[1], warmth_ext_formatted$se_formatted[1],
               warmth_ext_formatted$formatted[2], warmth_ext_formatted$se_formatted[2])
)
attr(rows_extremism, "position") <- c(1, 2, 3, 4)
```

Politically extreme participants showed greater bias about the outgroup at baseline (*b* = `r sprintf("%.3f", extremism_acc_baseline_b)`, *r* = `r sprintf("%.2f", extremism_acc_baseline_r)`, *p* `r if(extremism_acc_baseline_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_acc_baseline_p))`) and felt less warm toward them (*b* = `r sprintf("%.2f", extremism_warmth_baseline_b)`, *r* = `r sprintf("%.2f", extremism_warmth_baseline_r)`, *p* `r if(extremism_warmth_baseline_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_warmth_baseline_p))`).

The intervention was as successful in increasing accuracy and warmth in more extreme partisans as it was for more moderate ones.
The three-way interaction for accuracy was `r if(extremism_3way_acc_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.3f", extremism_3way_acc_coef)`, *p* `r if(extremism_3way_acc_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_3way_acc_p))`), and for warmth was `r if(extremism_3way_warmth_p < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.2f", extremism_3way_warmth_coef)`, *p* `r if(extremism_3way_warmth_p < 0.001) "< .001" else paste("=", sprintf("%.3f", extremism_3way_warmth_p))`).

```{r}
#| label: tbl-extremism
#| tbl-cap: "Tests of whether political extremism moderates belief updating. Upper panel shows simple effects of time at low extremism (-1 SD) and high extremism (+1 SD) levels. Lower panel shows full mixed-effects regressions with three-way interactions (Time × Learner Party × Political Extremism). While politically extreme participants showed greater baseline bias, extremism did not significantly moderate the effectiveness of the chatbot intervention."
# Display models
tbl <- modelsummary(
  list("Accuracy" = accuracy_extremism_model, "Warmth" = warmth_extremism_model),
  stars = TRUE,
  add_rows = rows_extremism,
  coef_omit = "Intercept",
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Time effects show pre-to-post change at Low Extremism (-1 SD) and High Extremism (+1 SD). Three-way interaction tests whether extremism moderates differential updating. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  strip_tt(line = TRUE) %>%
  group_tt(i = list("Simple Effects by Extremism Level" = 1, "Full Regression Results" = 5)) %>%
  style_tt(i = c(1, 6), bold = TRUE) %>%
  style_tt(i = c(20, 21), line = "b", line_color = "lightgray")
save_table(tbl, "tab_q3_extremism")
```

### Is belief updating associated with how the chatbot is perceived?

```{r process-models}
# Prepare data for regressions
dat_for_reg <- dat %>%
  filter(!is.na(accuracy_pre), !is.na(accuracy_post),
         !is.na(warmth_outgroup_pre), !is.na(warmth_outgroup_post),
         !is.na(info), !is.na(empathy))

# Process measure regressions
accuracy_process_model <- lm(accuracy_post ~ accuracy_pre + info + empathy, data = dat_for_reg)
warmth_process_model <- lm(warmth_outgroup_post ~ warmth_outgroup_pre + info + empathy, data = dat_for_reg)

# Extract coefficients for inline reporting
info_acc_b <- coef(accuracy_process_model)["info"]
info_acc_p <- summary(accuracy_process_model)$coefficients["info", "Pr(>|t|)"]
empathy_acc_b <- coef(accuracy_process_model)["empathy"]
empathy_acc_p <- summary(accuracy_process_model)$coefficients["empathy", "Pr(>|t|)"]

info_warmth_b <- coef(warmth_process_model)["info"]
info_warmth_p <- summary(warmth_process_model)$coefficients["info", "Pr(>|t|)"]
empathy_warmth_b <- coef(warmth_process_model)["empathy"]
empathy_warmth_p <- summary(warmth_process_model)$coefficients["empathy", "Pr(>|t|)"]

# Calculate correlation between empathy and informativeness
empathy_info_cor <- cor.test(dat_for_reg$empathy, dat_for_reg$info)
empathy_info_r <- empathy_info_cor$estimate
empathy_info_p <- empathy_info_cor$p.value
```

Participants who found the chatbot more informative showed `r if(info_acc_p < 0.05) "significantly" else "marginally"` better accuracy improvement (*b* = `r sprintf("%.3f", info_acc_b)`, *p* `r pformat(info_acc_p)`) and `r if(info_warmth_p < 0.05) "significantly" else "marginally"` greater warmth increases (*b* = `r sprintf("%.2f", info_warmth_b)`, *p* `r pformat(info_warmth_p)`).
Bot empathy did not significantly predict accuracy change (*b* = `r sprintf("%.3f", empathy_acc_b)`, *p* `r pformat(empathy_acc_p)`) or warmth change (*b* = `r sprintf("%.2f", empathy_warmth_b)`, *p* `r pformat(empathy_warmth_p)`). Perceptions of bot empathy and informativeness correlated positively but modestly (*r* = `r sprintf("%.2f", empathy_info_r)`, *p* `r pformat(empathy_info_p)`), suggesting they captured partially distinct aspects of chatbot interaction quality (@fig-empathy-informativeness). 

```{r}
#| label: tbl-mechanisms
#| tbl-cap: "OLS regressions testing whether chatbot informativeness and empathy predict post-interaction accuracy and warmth, controlling for baseline levels. Bot informativeness significantly predicted both improved accuracy and increased warmth toward the outgroup, while perceived empathy did not significantly predict either outcome."
# Display models
tbl <- modelsummary(
  list("Accuracy" = accuracy_process_model, "Warmth" = warmth_process_model),
  stars = TRUE,
  gof_map = c("nobs", "r.squared", "adj.r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Regressions control for pre-interaction values. * p<0.05, ** p<0.01, *** p<0.001"
)
save_table(tbl, "tab_q4_process")
```

```{r}
#| label: fig-empathy-informativeness
#| fig-cap: "Correlation between perceived chatbot empathy and informativeness. Points are jittered to reduce overplotting. The two dimensions showed modest positive correlations (r approximately .30), suggesting they capture partially distinct aspects of chatbot interaction quality. Correlation statistics displayed separately by learner party."
#| fig-width: 6
#| fig-height: 4
p <- dat_for_reg %>%
  ggplot(aes(x = empathy, y = info, color = learner_party)) +
  geom_jitter(alpha = 0.6, size = 2.5, width = 0.3, height = 0.3) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Empathy Rating (1-5)",
    y = "Informativeness Rating (1-5)"
  ) +
  theme_minimal()+
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_empathy_info.png"), p, width = 6, height = 4, dpi = 300)
p
```

### Does engagement ("dose") matter for belief updating?

```{r engagement-descriptives}
# Descriptive statistics for engagement
engagement_summary <- dat %>%
  summarise(
    mean_turns = mean(user_turn_count, na.rm = TRUE),
    sd_turns = sd(user_turn_count, na.rm = TRUE),
    mean_words = mean(user_word_count, na.rm = TRUE),
    sd_words = sd(user_word_count, na.rm = TRUE)
  )

# Create change scores and log-transformed word count
# No +1 needed for log transform since min word count = 47
dat <- dat %>%
  mutate(
    accuracy_change = accuracy_post - accuracy_pre,  # Negative = improvement
    warmth_change = warmth_outgroup_post - warmth_outgroup_pre,
    log_word_count = log(user_word_count)
  )
```

For the most part, the *dose* of the intervention that participants chose to take was unrelated to how much they updated their beliefs, or to whether they became more warm.

Participants engaged in an average of `r sprintf("%.2f", engagement_summary$mean_turns)` conversation turns (*SD* = `r sprintf("%.2f", engagement_summary$sd_turns)`) and typed an average of `r sprintf("%.2f", engagement_summary$mean_words)` words (*SD* = `r sprintf("%.2f", engagement_summary$sd_words)`).

```{r engagement-models}
# Model: Engagement predicting accuracy and warmth change

# TO-DO add models with the interaction term between engagemnt (turn or word count) with participant party. add simple effects at the top of the table.
m_accuracy_turns <- lm(accuracy_change ~ user_turn_count + accuracy_pre + learner_party,
                       data = dat)
m_accuracy_log_words <- lm(accuracy_change ~ log_word_count + accuracy_pre + learner_party,
                           data = dat)
m_warmth_turns <- lm(warmth_change ~ user_turn_count + warmth_outgroup_pre + learner_party,
                     data = dat)
m_warmth_log_words <- lm(warmth_change ~ log_word_count + warmth_outgroup_pre + learner_party,
                         data = dat)
```

Greater engagement—measured by turn count—was `r if(coef(summary(m_accuracy_turns))["user_turn_count", "Pr(>|t|)"] < 0.05) "significantly" else "not significantly"` associated with accuracy change (*b* = `r sprintf("%.4f", coef(m_accuracy_turns)["user_turn_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_accuracy_turns))["user_turn_count", "Pr(>|t|)"])`), and `r if(coef(summary(m_warmth_turns))["user_turn_count", "Pr(>|t|)"] < 0.05) "significantly" else "not significantly"` associated with warmth change (*b* = `r sprintf("%.4f", coef(m_warmth_turns)["user_turn_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_warmth_turns))["user_turn_count", "Pr(>|t|)"])`).

Using log-transformed word count, the effect on accuracy was `r if(coef(summary(m_accuracy_log_words))["log_word_count", "Pr(>|t|)"] < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.4f", coef(m_accuracy_log_words)["log_word_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_accuracy_log_words))["log_word_count", "Pr(>|t|)"])`), and on warmth was `r if(coef(summary(m_warmth_log_words))["log_word_count", "Pr(>|t|)"] < 0.05) "significant" else "not significant"` (*b* = `r sprintf("%.4f", coef(m_warmth_log_words)["log_word_count"])`, *p* = `r sprintf("%.3f", coef(summary(m_warmth_log_words))["log_word_count", "Pr(>|t|)"])`).

The only exception to the pattern, shown in @fig-engagement, was that Democrat partisans who wrote more words to the chatbot were more likely to increase in accuracy, but this was not true for turn count.

```{r}
#| label: tbl-engagement
#| tbl-cap: "OLS regressions testing whether conversation engagement (turn count and log word count) predicts changes in accuracy and warmth, controlling for baseline levels and learner party. Models test whether greater dose of interaction produced larger belief updates. Engagement showed mixed associations with outcomes."
# Consolidated table
tbl <- modelsummary(
  list(
    "Turn Count" = m_accuracy_turns,
    "Log(Word Count)" = m_accuracy_log_words,
    "Turn Count" = m_warmth_turns,
    "Log(Word Count)" = m_warmth_log_words
  ),
  stars = TRUE,
  gof_map = c("nobs", "r.squared", "adj.r.squared"),
  coef_map = coef_map_common
)  %>%
  group_tt(j = list("Accuracy" = 2:3, "Warmth" = 4:5))
save_table(tbl, "tab_q5_engagement")
```

```{r engagement-plots}
#| label: fig-engagement
#| fig-cap: "Relationships between conversation engagement and belief updating. Top row shows associations between turn count and changes in accuracy (left) and warmth (right). Bottom row shows associations between log-transformed word count and the same outcomes. Points represent individual participants, with outliers exceeding 75 turns excluded. Correlation statistics and linear regression lines displayed separately by learner party. Negative values on accuracy change indicate improvement."
#| fig-width: 10
#| fig-height: 8

# Filter out outliers with over 75 turns
dat_filtered <- dat %>% filter(user_turn_count <= 75)

# Plots with turn count
p1 <- ggplot(dat_filtered, aes(x = user_turn_count, y = accuracy_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  stat_cor()+
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Turn Count",
    y = "Accuracy Change\n(Negative = Improvement)"
  ) +
  theme_minimal()

p2 <- ggplot(dat_filtered, aes(x = user_turn_count, y = warmth_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  stat_cor()+
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Turn Count",
    y = "Warmth Change\n(Positive = Increase)"
  ) +
  theme_minimal()

# Plots with log word count
p3 <- ggplot(dat_filtered, aes(x = log_word_count, y = accuracy_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  stat_cor()+
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Log(Word Count)",
    y = "Accuracy Change\n(Negative = Improvement)"
  ) +
  theme_minimal()

p4 <- ggplot(dat_filtered, aes(x = log_word_count, y = warmth_change, color = learner_party)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  stat_cor()+
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Log(Word Count)",
    y = "Warmth Change\n(Positive = Increase)"
  ) +
  theme_minimal()

# Combine plots with single shared legend at bottom
p_combined <- (p1 + p2) / (p3 + p4) +
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom")
ggsave(paste0(fig_path, "fig_engagement.png"), p_combined, width = 10, height = 8, dpi = 300)
p_combined
```

### Does confidence track accuracy—or diverge from it?

We examine participants' metacognitive awareness by testing (1) whether confidence changes from pre to post, (2) The relationship between accuracy and confidence,  and (3) whether confidence changes track with actual accuracy improvements.
#### Does confidence change pre-to-post?

```{r confidence-model}
# Prepare confidence data in long format
confidence_long <- dat %>%
  select(ResponseId, learner_party, confidence_outgroup_pre, confidence_outgroup_post) %>%
  pivot_longer(cols = c(confidence_outgroup_pre, confidence_outgroup_post),
               names_to = "time", names_pattern = "confidence_outgroup_(pre|post)",
               values_to = "confidence") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

# Calculate summary statistics for plot
confidence_summary <- confidence_long %>%
  group_by(learner_party, time) %>%
  summarise(mean_confidence = mean(confidence, na.rm = TRUE),
            se_confidence = sd(confidence, na.rm = TRUE) / sqrt(n()),
            .groups = "drop")



# Mixed-effects models
confidence_model0 <- lmer(confidence ~ time + (1 | ResponseId), data = confidence_long)
confidence_model <- lmer(confidence ~ time * learner_party + (1 | ResponseId), data = confidence_long)

# Get simple effects for the interaction model
confidence_contrasts <- emmeans(confidence_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Extract values for inline reporting
conf_dr_est <- confidence_contrasts$estimate[1]  # D→R effect
conf_dr_p <- confidence_contrasts$p.value[1]
conf_rd_est <- confidence_contrasts$estimate[2]  # R→D effect
conf_rd_p <- confidence_contrasts$p.value[2]

# Calculate Cohen's d for confidence changes by learner party
conf_dr_d <- cohens_d(confidence ~ time, data = confidence_long %>% filter(learner_party == "D→R"), pooled_sd = FALSE)$Cohens_d
conf_rd_d <- cohens_d(confidence ~ time, data = confidence_long %>% filter(learner_party == "R→D"), pooled_sd = FALSE)$Cohens_d

# Format simple effects
conf_formatted <- confidence_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

rows_confidence <- tibble(
  term = c("Time effect (D→R)", "", "Time effect (R→D)", ""),
  `Confidence (Main Effect)` = c("", "", "", ""),
  `Confidence (Interaction)` = c(conf_formatted$formatted[1], conf_formatted$se_formatted[1],
                                 conf_formatted$formatted[2], conf_formatted$se_formatted[2])
)
attr(rows_confidence, "position") <- c(1, 2, 3, 4)
```

Republicans learning about Democrats gained confidence after the interaction (*b* = `r sprintf("%.3f", conf_rd_est)`, *d* = `r sprintf("%.2f", conf_rd_d)`, *p* `r if(conf_rd_p < 0.001) "< .001" else paste("=", sprintf("%.3f", conf_rd_p))`), despite learning less than Democrats.
Democrats' confidence did not significantly change (*b* = `r sprintf("%.3f", conf_dr_est)`, *d* = `r sprintf("%.2f", conf_dr_d)`, *p* `r if(conf_dr_p < 0.001) "< .001" else paste("=", sprintf("%.3f", conf_dr_p))`).

```{r}
#| label: fig-confidence-change
#| fig-cap: "Changes in confidence in outgroup judgments from pre- to post-interaction. Republicans learning about Democrats showed significant increases in confidence, while Democrats learning about Republicans showed no significant change. Error bars represent standard errors."
#| fig-width: 6
#| fig-height: 4
# Confidence plot
p <- ggplot(confidence_summary, aes(x = time, y = mean_confidence, color = learner_party, group = learner_party)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_confidence - se_confidence, ymax = mean_confidence + se_confidence), width = 0.1) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(x = "Time", y = "Mean Confidence (1-5)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")
ggsave(paste0(fig_path, "fig_confidence.png"), p, width = 6, height = 4, dpi = 300)
p

```{r}
#| label: tbl-confidence
#| tbl-cap: "Mixed-effects models testing changes in confidence in outgroup judgments. Upper panel shows simple effects of time separately by learner party (D→R and R→D). Lower panel shows full regression results with Time × Learner Party interaction. Republicans gained confidence after the interaction, while Democrats did not, suggesting asymmetric metacognitive responses."
# Display models
tbl <- modelsummary(
  list("Confidence (Main Effect)" = confidence_model0,
       "Confidence (Interaction)" = confidence_model),
  stars = TRUE,
  add_rows = rows_confidence,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Confidence rated on 1-5 scale. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  strip_tt(line = TRUE) %>%
  group_tt(i = list("Simple Effects by Political Affiliation" = 1, "Full Regression Results" = 5)) %>%
  style_tt(i = c(1, 6), bold = TRUE) %>%
  style_tt(i = c(14, 16), line = "b", line_color = "lightgray")
save_table(tbl, "tab_q7_confidence")
```



#### Relationship between confidence and accuracy

As shown in the figure below, Democrats show confidence patterns consistent with the Dunning-Kruger effect at baseline (i.e., less accurate participants are *more* confident, even when they are wrong; more accurate participants are less confident). After the intervention (dashed line), more accurate participants gain confidence. Higher skilled Republican participants are more correctly calibrated on their confidence about Democrats.

```{r baseline-accuracy-confidence-correlation}
#| label: fig-dunning-kruger
#| fig-cap: "Relationship between skill and confidence in outgroup judgments before and after chatbot interaction. Solid lines show pre-interaction patterns, dashed lines show post-interaction patterns. Less skilled participants (those with lower accuracy) exhibited higher confidence at baseline, consistent with Dunning-Kruger effects. Lines represent LOESS smoothers with 95% confidence bands."
#| fig-width: 8
#| fig-height: 4

# Create inverted accuracy for plotting (higher = better skill)
dat_dk <- dat %>%
  mutate(
    skill_pre = accuracy_pre, 
    skill_post = accuracy_post
  ) %>%
  select(ResponseId, participant_party, skill_pre, skill_post,
         confidence_outgroup_pre, confidence_outgroup_post) %>%
  pivot_longer(cols = c(skill_pre, skill_post, confidence_outgroup_pre, confidence_outgroup_post),
               names_to = c(".value", "time"),
               names_pattern = "(.+)_(pre|post)") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

p <- ggplot(dat_dk, aes(x = skill, y = confidence_outgroup, linetype = time, color = participant_party, fill = participant_party)) +
  geom_smooth(method = "loess", se = TRUE, linewidth = 1, alpha = .1) +
  facet_wrap(~participant_party) +
  scale_linetype_manual(values = c("pre" = "solid", "post" = "dashed"),
                        labels = c("pre" = "Pre-interaction", "post" = "Post-interaction"),
                        name = "Time") +
  labs(
    x = "Skill (Higher = More Accurate)",
    y = "Confidence (1-5 scale)"
  ) +
  theme_minimal() +
  scale_color_manual(values = party_colors)+
  scale_fill_manual(values = party_colors)+
  theme(legend.position = "none",
        strip.text = element_text(face = "bold"))
ggsave(paste0(fig_path, "fig_dunning_kruger.png"), p, width = 8, height = 4, dpi = 300)
p
```

#### Does confidence change track accuracy improvement?

```{r confidence-calibration, fig.width=8, fig.height=6}
# Compute change scores
calibration_dat <- dat_for_reg %>%
  mutate(
    confidence_change = confidence_outgroup_post - confidence_outgroup_pre,
    accuracy_change = accuracy_post - accuracy_pre  # Negative = improvement
  )

# Compute correlations by group
cor_overall <- cor.test(calibration_dat$confidence_change, calibration_dat$accuracy_change)
cor_dr <- cor.test(
  calibration_dat$confidence_change[calibration_dat$learner_party == "D→R"],
  calibration_dat$accuracy_change[calibration_dat$learner_party == "D→R"]
)
cor_rd <- cor.test(
  calibration_dat$confidence_change[calibration_dat$learner_party == "R→D"],
  calibration_dat$accuracy_change[calibration_dat$learner_party == "R→D"]
)
```

Overall, confidence change did not track with accuracy improvement (*r* = `r sprintf("%.3f", cor_overall$estimate)`, *p* `r if(cor_overall$p.value < 0.001) "< .001" else paste("=", sprintf("%.3f", cor_overall$p.value))`). For Democrats learning about Republicans, there was a `r if(cor_dr$p.value < 0.05) "significant positive" else "positive"` correlation between confidence change and accuracy change (*r* = `r sprintf("%.3f", cor_dr$estimate)`, *p* `r if(cor_dr$p.value < 0.001) "< .001" else paste("=", sprintf("%.3f", cor_dr$p.value))`), indicating that those who became less accurate gained more confidence. For Republicans learning about Democrats, confidence change did not correlate with accuracy change (*r* = `r sprintf("%.3f", cor_rd$estimate)`, *p* `r if(cor_rd$p.value < 0.001) "< .001" else paste("=", sprintf("%.3f", cor_rd$p.value))`).

```{r}
#| label: fig-confidence-calibration
#| fig-cap: "Metacognitive calibration showing relationship between confidence change and accuracy change. Points represent individual participants with jittering to reduce overplotting. Quadrant labels indicate well-calibrated (rightfully more/less confident) versus miscalibrated (wrongly more/less confident) changes. Negative values on y-axis indicate accuracy improvement. For Democrats (D→R), increased confidence was paradoxically associated with becoming less accurate, suggesting poor metacognitive calibration. Dashed lines mark no-change thresholds."
#| fig-width: 8
#| fig-height: 6
# Create scatter plot with correlation statistics
p <- ggplot(calibration_dat, aes(x = confidence_change, y = accuracy_change, color = learner_party)) +
  geom_jitter(alpha = 0.6, size = 2.5, width = .2, height= 0) +
  geom_smooth(method = "lm", se = TRUE, linewidth = 1) +
  stat_cor(aes(label = paste((..r.label..), ..p.label.., sep = "~`,`~")),
           method = "pearson", show.legend = FALSE, size = 3.5) +
  scale_color_manual(values = learner_party_colors, name = "Learner Party") +
  labs(
    x = "Confidence Change (Post - Pre)",
    y = "Accuracy Change (Post - Pre)\nNegative = Improvement"
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5)+
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5)+
  annotate(geom = 'text', x = 2.5, y = -2.5, label= 'Rightfully\nmore confident', size = 3) +
  annotate(geom = 'text', x = -2.5, y = -2.5, label= 'Rightfully\nless confident', size = 3) +
  annotate(geom = 'text', x = 2.5, y = 2.5, label= 'Wrongly\nmore confident', size = 3) +
  annotate(geom = 'text', x = -2.5, y = 2.5, label= 'Wrongly\nless confident', size = 3) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
ggsave(paste0(fig_path, "fig_confidence_calibration.png"), p, width = 8, height = 6, dpi = 300)
p
```


## Discussion

### Summary of Findings

Brief interactions with an AI chatbot representing a political outgroup improved accuracy of beliefs about that outgroup and increased warmth toward them. Participants became more accurate in estimating their political outgroup's environmental attitudes and reported warmer feelings toward the outgroup on a feeling thermometer. These effects were asymmetric: Democrats learning about Republicans showed greater accuracy improvements than Republicans learning about Democrats, though both groups increased warmth at similar rates.

Political extremism predicted greater baseline bias and lower baseline warmth but did not moderate the intervention's effectiveness. More and less extreme participants benefited equally from the interaction. Perceptions of the chatbot's informativeness predicted better outcomes, while perceived empathy did not. Greater engagement, measured by conversation length, showed mixed associations with belief updating.

Confidence changes did not track with accuracy improvements. Republicans learning about Democrats gained confidence despite smaller accuracy gains, while Democrats' confidence did not change. For Democrats, confidence increases were paradoxically associated with becoming less accurate, suggesting poor metacognitive calibration in this context.

### Implications for Marketing Practice

These findings suggest that AI-mediated interactions can serve as a scalable tool for reducing political misperceptions in consumer research. Marketers often struggle to understand ideologically diverse consumer segments, relying on stereotypes or costly focus groups. AI chatbots could provide an accessible method for gaining insight into outgroup consumer preferences and values.

This approach may be particularly valuable for marketing environmentally responsible products, where political divides can lead to misperceptions about consumer attitudes. Understanding that political outgroups may be more environmentally conscious than stereotypes suggest could inform more effective cross-partisan messaging strategies.

### Limitations

This study has several limitations that provide directions for future research.

**Domain specificity.** The environmental attitude domain may not generalize to other consumer preferences or policy areas. Environmental attitudes represent a domain where partisan divides exist but are not as stark as other politically charged topics. Future research should examine whether AI-mediated learning transfers to domains with deeper partisan disagreement, such as immigration, healthcare, or gun policy. The effectiveness of this intervention may vary depending on the perceived relevance of the attitude domain to partisan identity.

**Temporal dynamics.** This study examines only immediate post-interaction effects. The durability of accuracy improvements and warmth increases remains unknown. Research on intergroup contact suggests that initial attitude changes can decay over time, particularly without repeated exposure. Future studies should include delayed follow-up measurements to assess whether belief updates persist days, weeks, or months after the interaction. Understanding decay rates would inform practical recommendations about intervention frequency for sustained effects.

**AI representation versus human reality.** Participants interacted with an AI chatbot prompted to represent a political outgroup rather than actual outgroup members. While this approach offers scalability advantages, it raises questions about whether insights gained from AI interactions transfer to understanding real outgroup members. The chatbot was prompted to represent the outgroup but may not fully capture the diversity of views within each political group. Individual variation within partisan groups is substantial, and the AI's responses may reinforce stereotypes about group homogeneity even as they correct specific attitude misperceptions. Future research should examine whether learning from AI translates to more accurate perceptions of actual individuals and whether AI-mediated learning produces different outcomes than human-mediated contact.

**Comparison to alternative interventions.** This study does not compare the chatbot intervention to other approaches for reducing partisan misperceptions. Standard perspective-taking interventions have shown some effectiveness in reducing intergroup bias in the psychological literature. Information provision through simple fact sheets or Google searches might achieve similar accuracy improvements at lower cost. Interactions with digital twins based on actual individuals' response patterns could combine scalability with greater authenticity. Future research should directly compare these alternatives to determine the relative effectiveness and cost-efficiency of AI chatbots versus other scalable interventions for partisan debiasing.



## Methods

```{r demographics}
# Calculate demographic statistics
age_mean <- mean(dat$age, na.rm = TRUE)
age_sd <- sd(dat$age, na.rm = TRUE)
age_range <- range(dat$age, na.rm = TRUE)

# Gender breakdown
gender_counts <- dat %>%
  count(gender) %>%
  mutate(pct = n / sum(n) * 100)

pct_female <- gender_counts %>% filter(gender == "Female") %>% pull(pct)
pct_male <- gender_counts %>% filter(gender == "Male") %>% pull(pct)
pct_other_gender <- 100 - pct_female - pct_male

# Race/ethnicity (participants could select multiple)
pct_white <- mean(dat$race_white == 1, na.rm = TRUE) * 100
pct_black <- mean(dat$race_black == 1, na.rm = TRUE) * 100
pct_latino <- mean(dat$race_latino == 1, na.rm = TRUE) * 100
pct_asian <- mean(dat$race_asian == 1, na.rm = TRUE) * 100
pct_other_race <- mean((dat$race_native_american == 1 | dat$race_pacific_islander == 1 | dat$race_other == 1), na.rm = TRUE) * 100
```

### Participants

We recruited `r nrow(dat)` participants via Qualtrics panels. Participants were prescreened to exclude political moderates, retaining only those with clear partisan identities. Specifically, we recruited participants who scored below 25 or above 75 on a 0-100 political orientation scale during prescreening, where 0 represents strong Democratic identification and 100 represents strong Republican identification. The final sample included `r n_dr` Democrats learning about Republicans (D→R) and `r n_rd` Republicans learning about Democrats (R→D).

Participants' mean age was `r sprintf("%.1f", age_mean)` years (*SD* = `r sprintf("%.1f", age_sd)`, range = `r age_range[1]`–`r age_range[2]`). The sample was `r sprintf("%.1f", pct_female)`% female, `r sprintf("%.1f", pct_male)`% male, and `r sprintf("%.1f", pct_other_gender)`% other gender identities. Racial/ethnic composition was `r sprintf("%.1f", pct_white)`% White, `r sprintf("%.1f", pct_black)`% Black or African American, `r sprintf("%.1f", pct_latino)`% Latino, `r sprintf("%.1f", pct_asian)`% Asian, and `r sprintf("%.1f", pct_other_race)`% other racial/ethnic identities (participants could select multiple categories).

Participants were compensated through the Qualtrics panel system. This research was approved by the University of Pennsylvania Institutional Review Board.

### Procedure

Participants completed a three-phase online study: baseline assessment, AI chatbot interaction, and post-interaction assessment.

**Phase 1: Baseline Assessment.** Participants first reported their own environmental attitudes by rating their agreement with six statements about environmental issues on a 5-point scale (1 = Disagree strongly, 5 = Agree strongly). They then estimated the average environmental attitudes of both Democrats and Republicans using the same six items. Next, participants rated their warmth toward Democrats and Republicans on a 0-100 feeling thermometer and indicated their confidence in their outgroup judgments on a 5-point scale (1 = Not at all confident, 5 = Extremely confident).

**Phase 2: AI Chatbot Interaction.** Participants engaged in a 10-minute conversation with an AI chatbot (Claude Sonnet 3.5) that was prompted to represent the political views of their outgroup. The chatbot was instructed to respond as a typical member of the opposing political party and to discuss environmental attitudes authentically. Participants could ask any questions they wished about the outgroup's environmental beliefs and values. The chatbot interaction was not a real person but an AI system designed to simulate realistic political perspectives.

**Phase 3: Post-Interaction Assessment.** After the conversation, participants re-estimated the outgroup's environmental attitudes using the same six items and re-rated their warmth toward the outgroup. They also indicated their confidence in these post-interaction judgments and rated the chatbot on two dimensions: informativeness and empathy (both on 5-point scales: 1 = Not at all, 5 = Extremely). Finally, participants wrote a marketing slogan for energy-saving appliances targeted at the political outgroup.

### Measures

We measured two primary outcomes to assess belief updating and affective change.

- **Belief accuracy**: Absolute difference between participants' estimates of outgroup green attitudes and actual outgroup means (lower = more accurate)
- **Outgroup warmth**: Feeling thermometer (0-100 scale) toward the political outgroup

Additionally, we collected confidence in outgroup judgments (5-point scale) and bot informativeness and empathy ratings (5-point scale)

### Pre-Registered Analyses

For each primary outcome, we estimated mixed-effects models of the form:

$$y \sim \text{time} \times \text{learner\_party} + (1 | \text{id})$$

where `time` indexes pre- versus post-interaction measurement and `learner_party` indicates D→R versus R→D learning direction.

## Appendix

### Does interacting with the chatbot improve applied judgment?

Participants wrote slogans for marketing energy-saving appliances to the political outgroup.
I haven't touched this data yet, because we don't have a control condition or a baseline slogan to make a meaningful comparison to.

**DATA AVAILABLE**: Raw slogan text in variable `slogan` (n = `r sum(!is.na(dat$slogan))` responses).

**DATA NEEDED**:
1. Semantic similarity scores (compare to AI-generated responses to detect copying)
2. Quality ratings (human or LLM ratings of slogan fit to target outgroup)

```{r example-slogans}
# Get example slogans by party
set.seed(123)  # For reproducibility
example_slogans_dr <- dat %>%
  filter(learner_party == "D→R", !is.na(slogan)) %>%
  sample_n(min(10, n())) %>%
  select(slogan) %>%
  pull()

example_slogans_rd <- dat %>%
  filter(learner_party == "R→D", !is.na(slogan)) %>%
  sample_n(min(10, n())) %>%
  select(slogan) %>%
  pull()
```

#### Example Slogans

**Democrats Marketing to Republicans (D→R):**

```{r display-slogans-dr, results='asis'}
for (i in 1:length(example_slogans_dr)) {
  cat(paste0(i, ". \"", example_slogans_dr[i], "\"\n"))
}
```

**Republicans Marketing to Democrats (R→D):**

```{r display-slogans-rd, results='asis'}
for (i in 1:length(example_slogans_rd)) {
  cat(paste0(i, ". \"", example_slogans_rd[i], "\"\n"))
}
```

### Correlation Matrix

#### Overall Correlations

```{r}
#| label: tbl-correlation-matrix-overall
#| tbl-cap: "Overall correlation matrix with significance stars for all key variables. Lower triangle shows Pearson correlation coefficients with significance indicators (* p<0.05, ** p<0.01, *** p<0.001). Upper triangle intentionally left blank for readability. Variables include political orientation, belief accuracy, environmental attitudes, outgroup warmth, confidence, bot perceptions, and engagement measures."

# Function to compute correlation with significance stars
cor_with_stars <- function(x, y) {
  if (sum(!is.na(x) & !is.na(y)) < 3) return("")  # Need at least 3 observations
  test <- cor.test(x, y, use = "pairwise.complete.obs")
  r <- test$estimate
  p <- test$p.value

  stars <- case_when(
    p < 0.001 ~ "***",
    p < 0.01 ~ "**",
    p < 0.05 ~ "*",
    TRUE ~ ""
  )

  sprintf("%.2f%s", r, stars)
}

# Select key continuous variables for correlation matrix
cor_vars <- dat %>%
  select(
    # Political orientation
    `Political Orientation` = politicalorientation_1,
    `Political Extremism` = political_extremism,

    # Accuracy measures
    `Accuracy (Pre)` = accuracy_pre,
    `Accuracy (Post)` = accuracy_post,

    # Green attitudes
    `Green Own` = green_own,
    `Green Ingroup` = green_ingroup,
    `Green Outgroup (Pre)` = green_outgroup_pre,
    `Green Outgroup (Post)` = green_outgroup_post,

    # Warmth measures
    `Warmth Outgroup (Pre)` = warmth_outgroup_pre,
    `Warmth Outgroup (Post)` = warmth_outgroup_post,
    `Warmth Ingroup (Pre)` = warmth_ingroup_pre,
    `Warmth Ingroup (Post)` = warmth_ingroup_post,

    # Confidence measures
    `Confidence Ingroup` = confidence_ingroup,
    `Confidence Outgroup (Pre)` = confidence_outgroup_pre,
    `Confidence Outgroup (Post)` = confidence_outgroup_post,

    # Bot perceptions
    `Bot Informativeness` = info,
    `Bot Empathy` = empathy,

    # Engagement
    `Turn Count` = user_turn_count,
    `Word Count` = user_word_count
  )

# Create correlation matrix with stars
n_vars <- ncol(cor_vars)
var_names <- names(cor_vars)
cor_matrix_overall <- matrix("", nrow = n_vars, ncol = n_vars)
rownames(cor_matrix_overall) <- var_names
colnames(cor_matrix_overall) <- var_names

# Fill lower triangle with correlations and stars
for (i in 1:n_vars) {
  for (j in 1:n_vars) {
    if (i == j) {
      cor_matrix_overall[i, j] <- "—"  # Diagonal
    } else if (i > j) {
      cor_matrix_overall[i, j] <- cor_with_stars(cor_vars[[j]], cor_vars[[i]])
    }
    # Upper triangle stays empty
  }
}

# Convert to data frame and create gt table
cor_df_overall <- as.data.frame(cor_matrix_overall)
cor_df_overall <- cbind(Variable = rownames(cor_df_overall), cor_df_overall)
rownames(cor_df_overall) <- NULL

# Create gt table
tbl_overall <- cor_df_overall %>%
  gt() %>%
  tab_header(
    title = "Overall Correlation Matrix"
  ) %>%
  cols_label(
    Variable = ""
  ) %>%
  tab_footnote(
    footnote = "* p<0.05, ** p<0.01, *** p<0.001.
Lower triangle shows correlations with significance stars.
Upper triangle left blank for readability."
  ) %>%
  tab_style(
    style = cell_text(size = px(9)),
    locations = cells_body()
  ) %>%
  tab_style(
    style = cell_text(size = px(9), weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(size = px(9), weight = "bold"),
    locations = cells_body(columns = Variable)
  )

save_table(tbl_overall, "tab_correlation_matrix_overall")
```

#### Correlations by Political Party

```{r}
#| label: tbl-correlation-matrix
#| tbl-cap: "Correlation matrix with significance stars by political party. Upper triangle shows correlations for Republicans, lower triangle shows correlations for Democrats. Diagonal shows em-dashes. Significance indicators: * p<0.05, ** p<0.01, *** p<0.001. Variables include political orientation, belief accuracy, environmental attitudes, outgroup warmth, confidence, bot perceptions, and engagement measures."

# Function to compute correlation with significance stars
cor_with_stars <- function(x, y) {
  if (sum(!is.na(x) & !is.na(y)) < 3) return("")  # Need at least 3 observations
  test <- cor.test(x, y, use = "pairwise.complete.obs")
  r <- test$estimate
  p <- test$p.value

  stars <- case_when(
    p < 0.001 ~ "***",
    p < 0.01 ~ "**",
    p < 0.05 ~ "*",
    TRUE ~ ""
  )

  sprintf("%.2f%s", r, stars)
}

# Prepare data for Democrats and Republicans separately
dat_dem <- dat %>% filter(participant_party == "Democrat")
dat_rep <- dat %>% filter(participant_party == "Republican")

# Select key continuous variables for correlation matrix
select_cor_vars <- function(df) {
  df %>%
    select(
      # Political orientation
      `Political Orientation` = politicalorientation_1,
      `Political Extremism` = political_extremism,

      # Accuracy measures
      `Accuracy (Pre)` = accuracy_pre,
      `Accuracy (Post)` = accuracy_post,

      # Green attitudes
      `Green Own` = green_own,
      `Green Ingroup` = green_ingroup,
      `Green Outgroup (Pre)` = green_outgroup_pre,
      `Green Outgroup (Post)` = green_outgroup_post,

      # Warmth measures
      `Warmth Outgroup (Pre)` = warmth_outgroup_pre,
      `Warmth Outgroup (Post)` = warmth_outgroup_post,
      `Warmth Ingroup (Pre)` = warmth_ingroup_pre,
      `Warmth Ingroup (Post)` = warmth_ingroup_post,

      # Confidence measures
      `Confidence Ingroup` = confidence_ingroup,
      `Confidence Outgroup (Pre)` = confidence_outgroup_pre,
      `Confidence Outgroup (Post)` = confidence_outgroup_post,

      # Bot perceptions
      `Bot Informativeness` = info,
      `Bot Empathy` = empathy,

      # Engagement
      `Turn Count` = user_turn_count,
      `Word Count` = user_word_count
    )
}

cor_vars_dem <- select_cor_vars(dat_dem)
cor_vars_rep <- select_cor_vars(dat_rep)

# Create correlation matrix with stars
n_vars <- ncol(cor_vars_dem)
var_names <- names(cor_vars_dem)
cor_matrix <- matrix("", nrow = n_vars, ncol = n_vars)
rownames(cor_matrix) <- var_names
colnames(cor_matrix) <- var_names

# Fill matrix: Democrats in lower triangle, Republicans in upper triangle
for (i in 1:n_vars) {
  for (j in 1:n_vars) {
    if (i == j) {
      cor_matrix[i, j] <- "—"  # Diagonal
    } else if (i > j) {
      # Lower triangle: Democrats
      cor_matrix[i, j] <- cor_with_stars(cor_vars_dem[[j]], cor_vars_dem[[i]])
    } else {
      # Upper triangle: Republicans
      cor_matrix[i, j] <- cor_with_stars(cor_vars_rep[[j]], cor_vars_rep[[i]])
    }
  }
}

# Convert to data frame and create gt table
cor_df <- as.data.frame(cor_matrix)
cor_df <- cbind(Variable = rownames(cor_df), cor_df)
rownames(cor_df) <- NULL

# Create gt table with subtle shading to distinguish triangles
tbl <- cor_df %>%
  gt() %>%
  tab_header(
    title = "Correlation Matrix by Political Party",
    subtitle = "Upper Triangle: Republicans | Lower Triangle: Democrats"
  ) %>%
  cols_label(
    Variable = ""
  ) %>%
  tab_footnote(
    footnote = "* p<0.05, ** p<0.01, *** p<0.001"
  ) %>%
  tab_style(
    style = cell_text(size = px(9)),
    locations = cells_body()
  ) %>%
  tab_style(
    style = cell_text(size = px(9), weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(size = px(9), weight = "bold"),
    locations = cells_body(columns = Variable)
  )

save_table(tbl, "tab_correlation_matrix")
```

### Summary table of regressions
<!-- ::: {.landscape} -->
```{r}
#| label: tbl-appendix-consolidated
#| tbl-cap: "Consolidated summary of all models examining belief accuracy and outgroup warmth. Upper panels show simple effects at different levels of partisanship (D→R vs. R→D) and extremism (±1 SD). Lower panel shows full regression results. Models test main effects of time, interactions with learner party (partisanship), and moderation by political extremism."

# Calculate simple effects for partisanship
accuracy_party_contrasts <- emmeans(accuracy_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

warmth_party_contrasts <- emmeans(warmth_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Format simple effects for partisanship
acc_party_formatted <- accuracy_party_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

warmth_party_formatted <- warmth_party_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

# Calculate simple effects for extremism (already calculated above, reuse)
# These were calculated in the extremism section

# Create rows for simple effects
rows_party <- tibble(
  term = c("Time effect (D→R)", "", "Time effect (R→D)", ""),
  `2` = c(acc_party_formatted$formatted[1], acc_party_formatted$se_formatted[1],
          acc_party_formatted$formatted[2], acc_party_formatted$se_formatted[2]),
  `3` = c(acc_party_formatted$formatted[1], acc_party_formatted$se_formatted[1],
          acc_party_formatted$formatted[2], acc_party_formatted$se_formatted[2]),
  `4` = c("", "", "", ""),
  `5` = c("", "", "", ""),
  `6` = c(warmth_party_formatted$formatted[1], warmth_party_formatted$se_formatted[1],
          warmth_party_formatted$formatted[2], warmth_party_formatted$se_formatted[2]),
  `7` = c(warmth_party_formatted$formatted[1], warmth_party_formatted$se_formatted[1],
          warmth_party_formatted$formatted[2], warmth_party_formatted$se_formatted[2]),
  `8` = c("", "", "", ""),
  `9` = c("", "", "", "")
)
attr(rows_party, "position") <- c(1, 2, 3, 4)

rows_extremism_append <- tibble(
  term = c("Time effect (Low Extremism)", "", "Time effect (High Extremism)", ""),
  `2` = c("", "", "", ""),
  `3` = c("", "", "", ""),
  `4` = c(acc_ext_formatted$formatted[1], acc_ext_formatted$se_formatted[1],
          acc_ext_formatted$formatted[2], acc_ext_formatted$se_formatted[2]),
  `5` = c(acc_ext_formatted$formatted[1], acc_ext_formatted$se_formatted[1],
          acc_ext_formatted$formatted[2], acc_ext_formatted$se_formatted[2]),
  `6` = c("", "", "", ""),
  `7` = c("", "", "", ""),
  `8` = c(warmth_ext_formatted$formatted[1], warmth_ext_formatted$se_formatted[1],
          warmth_ext_formatted$formatted[2], warmth_ext_formatted$se_formatted[2]),
  `9` = c(warmth_ext_formatted$formatted[1], warmth_ext_formatted$se_formatted[1],
          warmth_ext_formatted$formatted[2], warmth_ext_formatted$se_formatted[2])
)
attr(rows_extremism_append, "position") <- c(5, 6, 7, 8)

rows_to_add = rbind(rows_party, rows_extremism_append)
attr(rows_to_add, "position") <- c(1:8)

tbl <- modelsummary(
  list(
    "Main Effect" = accuracy_model0,
    "Partisanship" = accuracy_model,
    "Extremism" = accuracy_extremism_model0,
    "Full Model" = accuracy_extremism_model,
    "Main Effect" = warmth_model0,
    "Partisanship" = warmth_model,
    "Extremism" = warmth_extremism_model0,
    "Full Model" = warmth_extremism_model
  ),
  stars = TRUE,
  add_rows = rows_to_add,
  gof_map = c("nobs", "r.squared"),
  coef_omit = "Intercept",
  statistic = NULL,
  coef_map = coef_map_common,
  notes = "Note: Simple effects show pre-to-post change by learner party (D→R vs. R→D) and extremism level (±1 SD). Full regression results below. * p<0.05, ** p<0.01, *** p<0.001") %>%
  group_tt(j = list(
    "Accuracy" = 2:5,
    "Warmth" = 6:9
  )) %>%
  group_tt(i = list(
    "Simple Effects by Learner Party" = 1,
    "Simple Effects by Extremism Level" = 5,
    "Full Regression Results" = 9
  )) %>%
    strip_tt(line = TRUE) %>%
  style_tt(i = c(1, 6, 11), bold = TRUE) %>%
  style_tt(i = c(5, 10, 18), line = "b", line_color = "lightgray") %>% 
    tinytable::style_tt(
  tabularray_inner = "rowsep=2pt, colsep=3pt"
) %>% 
theme_tt("resize", width = 1)
save_table(tbl, "tab_appendix_consolidated")
```
<!-- ::: -->

### Descriptive Statistics

```{r descriptives-actual-attitudes}
#| label: tbl-descriptives-green
#| tbl-cap: "Actual environmental attitudes by political party. Self-reported green attitudes measured on a 1-5 scale, with higher values indicating more pro-environmental attitudes. Democrats and Republicans showed relatively similar baseline environmental values."
# Actual environmental attitudes by party
dat %>%
  group_by(participant_party) %>%
  summarise(
    mean_green = mean(green_own, na.rm = TRUE),
    sd_green = sd(green_own, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    participant_party = "Political Party",
    mean_green = "Mean Green Attitude",
    sd_green = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_green, sd_green), decimals = 2)
```

```{r descriptives-accuracy}
#| label: tbl-descriptives-accuracy
#| tbl-cap: "Outgroup belief accuracy by learner party and time point. Values represent absolute error between participants' estimates of outgroup green attitudes and actual outgroup means, with lower values indicating more accurate beliefs. Both groups showed improvements from pre- to post-interaction."
# Accuracy by time and learner party
accuracy_long %>%
  group_by(learner_party, time) %>%
  summarise(
    mean_error = mean(error, na.rm = TRUE),
    sd_error = sd(error, na.rm = TRUE),
    n = sum(!is.na(error)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    time = "Time",
    mean_error = "Mean Error",
    sd_error = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_error, sd_error), decimals = 3)
```

```{r descriptives-warmth}
#| label: tbl-descriptives-warmth
#| tbl-cap: "Outgroup warmth by learner party and time point. Warmth measured using a feeling thermometer on a 0-100 scale, with higher values indicating warmer feelings toward the political outgroup. Both groups showed increases in warmth from pre- to post-interaction."
# Warmth by time and learner party
warmth_long %>%
  group_by(learner_party, time) %>%
  summarise(
    mean_warmth = mean(warmth, na.rm = TRUE),
    sd_warmth = sd(warmth, na.rm = TRUE),
    n = sum(!is.na(warmth)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    time = "Time",
    mean_warmth = "Mean Warmth",
    sd_warmth = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_warmth, sd_warmth), decimals = 2)
```

```{r descriptives-extremism}
#| label: tbl-descriptives-extremism
#| tbl-cap: "Political extremism by learner party. Extremism operationalized as absolute distance from the political center (50 on the 0-100 political orientation scale), ranging from 0 (centrist) to 50 (maximum extremism). Both Democrats and Republicans showed similar levels of extremism in this sample."
# Political extremism by learner party
dat %>%
  group_by(learner_party) %>%
  summarise(
    mean_extremism = mean(political_extremism, na.rm = TRUE),
    sd_extremism = sd(political_extremism, na.rm = TRUE),
    min_extremism = min(political_extremism, na.rm = TRUE),
    max_extremism = max(political_extremism, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    mean_extremism = "Mean",
    sd_extremism = "SD",
    min_extremism = "Min",
    max_extremism = "Max"
  ) %>%
  fmt_number(columns = c(mean_extremism, sd_extremism, min_extremism, max_extremism), decimals = 2)
```

```{r descriptives-bot-perceptions}
#| label: tbl-descriptives-bot
#| tbl-cap: "Chatbot perception ratings by learner party. Participants rated the chatbot's informativeness (how much they learned) and empathy (how empathetic it was) on 1-5 scales immediately after the interaction. Ratings were generally positive across both dimensions and political groups."
# Bot perceptions by learner party
dat %>%
  select(learner_party, info, empathy) %>%
  pivot_longer(cols = c(info, empathy), names_to = "measure", values_to = "rating") %>%
  mutate(measure = case_when(
    measure == "info" ~ "Informativeness",
    measure == "empathy" ~ "Empathy",
    TRUE ~ measure
  )) %>%
  group_by(learner_party, measure) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    n = sum(!is.na(rating)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    measure = "Measure",
    mean_rating = "Mean Rating",
    sd_rating = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_rating, sd_rating), decimals = 2)
```

```{r descriptives-confidence}
#| label: tbl-descriptives-confidence
#| tbl-cap: "Confidence in outgroup judgments by learner party and time point. Confidence measured on a 1-5 scale asking how confident participants felt about their understanding of the political outgroup. Republicans showed increases in confidence from pre- to post-interaction, while Democrats did not."
# Confidence by time and learner party
dat %>%
  select(ResponseId, learner_party, confidence_outgroup_pre, confidence_outgroup_post) %>%
  pivot_longer(cols = c(confidence_outgroup_pre, confidence_outgroup_post),
               names_to = "time", names_pattern = "confidence_outgroup_(pre|post)",
               values_to = "confidence") %>%
  mutate(time = factor(time, levels = c("pre", "post"))) %>%
  group_by(learner_party, time) %>%
  summarise(
    mean_confidence = mean(confidence, na.rm = TRUE),
    sd_confidence = sd(confidence, na.rm = TRUE),
    n = sum(!is.na(confidence)),
    .groups = "drop"
  ) %>%
  gt() %>%
  cols_label(
    learner_party = "Learner Party",
    time = "Time",
    mean_confidence = "Mean Confidence",
    sd_confidence = "SD",
    n = "N"
  ) %>%
  fmt_number(columns = c(mean_confidence, sd_confidence), decimals = 2)
```

### Robustness Checks

#### Warmth Difference Score

Following common practice in the affective polarization literature, we examine warmth using a difference score (outgroup - ingroup).
Effects replicate what is reported on the main text.

```{r warmth-difference}
# Prepare warmth difference data
warmth_diff_long <- dat %>%
  select(ResponseId, learner_party, warmth_diff_pre, warmth_diff_post) %>%
  pivot_longer(cols = c(warmth_diff_pre, warmth_diff_post), names_to = "time",
               names_pattern = "warmth_diff_(pre|post)", values_to = "warmth_diff") %>%
  mutate(time = factor(time, levels = c("pre", "post")))

# Mixed-effects model
warmth_diff_model <- lmer(warmth_diff ~ time * learner_party + (1 | ResponseId), data = warmth_diff_long)

# Get simple effects
warmth_diff_contrasts <- emmeans(warmth_diff_model, ~ time | learner_party) %>%
  pairs(reverse = TRUE) %>%
  tidy()

# Format simple effects
warmth_diff_formatted <- warmth_diff_contrasts %>%
  mutate(
    stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    formatted = sprintf("%.3f%s", estimate, stars),
    se_formatted = sprintf("(%.3f)", std.error)
  )

rows_warmth_diff <- tibble(
  term = c("Time effect (D→R)", "", "Time effect (R→D)", ""),
  `Warmth Difference` = c(warmth_diff_formatted$formatted[1], warmth_diff_formatted$se_formatted[1],
                          warmth_diff_formatted$formatted[2], warmth_diff_formatted$se_formatted[2])
)
attr(rows_warmth_diff, "position") <- c(1, 2, 3, 4)
```

```{r}
#| label: tbl-robustness-warmth-diff
#| tbl-cap: "Robustness check using warmth difference scores (outgroup minus ingroup warmth). This operationalization follows standard practice in affective polarization research. Upper panel shows simple effects by learner party. Lower panel shows full mixed-effects regression. Results replicate main findings, confirming that participants became less affectively polarized toward the political outgroup."
# Display model
modelsummary(
  list("Warmth Difference" = warmth_diff_model),
  stars = TRUE,
  add_rows = rows_warmth_diff,
  gof_map = c("nobs", "r.squared"),
  coef_map = coef_map_common,
  notes = "Note: Warmth difference = outgroup - ingroup. Positive values indicate reduced polarization. * p<0.05, ** p<0.01, *** p<0.001"
) %>%
  strip_tt(line = TRUE) %>%
  group_tt(i = list("Simple Effects by Political Affiliation" = 1, "Full Regression Results" = 5)) %>%
  style_tt(i = c(1, 6), bold = TRUE) %>%
  style_tt(i = c(12, 14), line = "b", line_color = "lightgray")
```

### Extreme Cases Analysis

To understand what characterizes successful versus unsuccessful learning, we examined participants who showed the most and least improvement in accuracy about their political outgroup.

```{r extreme-cases-analysis}
#| label: tbl-extreme-cases
#| tbl-cap: "Comparison of participants showing the most versus least improvement in accuracy. For each learner party, we identified the single participant who improved most and the single participant who worsened most (or improved least). Positive values indicate improvement (reduced error). Engagement metrics (word count, turn count) did not consistently distinguish successful from unsuccessful learners, suggesting quality of engagement matters more than quantity."
# Calculate accuracy improvement (negative = improvement)
extreme_cases <- dat %>%
  mutate(accuracy_improvement = accuracy_pre - accuracy_post) %>%
  group_by(learner_party) %>%
  arrange(desc(accuracy_improvement)) %>%
  slice(c(1, n())) %>%
  ungroup() %>%
  mutate(
    case_type = rep(c("Most Improved", "Least Improved"), 2)
  ) %>%
  select(ResponseId, learner_party, case_type,
         accuracy_pre, accuracy_post, accuracy_improvement,
         user_word_count, user_turn_count, slogan)

# Display summary table
extreme_cases %>%
  select(learner_party, case_type, accuracy_improvement, user_word_count, user_turn_count) %>%
  gt() %>%
  cols_label(
    learner_party = "Group",
    case_type = "Case",
    accuracy_improvement = "Accuracy Improvement",
    user_word_count = "Word Count",
    user_turn_count = "Turn Count"
  ) %>%
  fmt_number(columns = accuracy_improvement, decimals = 3) %>%
  fmt_number(columns = c(user_word_count, user_turn_count), decimals = 0) %>%
  tab_footnote(
    footnote = "Positive values indicate improvement (reduced error)",
    locations = cells_column_labels(columns = accuracy_improvement)
  )
```

#### Marketing Messages from Extreme Cases

Participants wrote marketing slogans for energy-saving appliances targeted at their political outgroup.
The messages reveal stark differences between successful and unsuccessful learners.

**Democrats Learning About Republicans (D→R)**

*Most Improved* (accuracy improvement = 2.843):
> "Energy-saving Appliances, Helps Make America Great!"

This message uses patriotic language that bridges partisan values.
The participant engaged deeply on economic issues and found genuine common ground.

*Least Improved* (accuracy worsened by 1.333):
> "made for independence 'lower your costs, stay in control'."

This message shows confused framing about independence and control.
The participant's conversation remained surface-level and distracted.

**Republicans Learning About Democrats (R→D)**

*Most Improved* (accuracy improvement = 3.000):
> "energy saving appliances . save energy and our planet"

This message authentically reflects Democratic environmental values with simple, accessible language.
The participant's conversation explored multiple policy areas comprehensively.

*Least Improved* (accuracy worsened by 2.167):
> "Come spend 50% more on less practical appliances since that's what you claim to do and support the business. But I know you won't because just like every Republican said its not financially plausible and your actions only prove the Republicans point."

This openly hostile message reinforces stereotypes rather than bridging divides.
The participant challenged the chatbot's authenticity throughout and remained adversarial.

#### Patterns in Successful vs. Unsuccessful Learning

```{r extreme-cases-engagement}
#| label: tbl-extreme-engagement
#| tbl-cap: "Engagement levels comparing successful versus unsuccessful learners among extreme cases. Based on n=4 participants (2 improved, 2 worsened). Participants who worsened actually showed slightly higher engagement on average, reinforcing that conversation quality matters more than quantity for belief updating."
# Compare engagement between successful and unsuccessful learners
engagement_comparison <- extreme_cases %>%
  mutate(
    success = ifelse(accuracy_improvement > 0, "Improved", "Worsened")
  ) %>%
  group_by(success) %>%
  summarise(
    mean_word_count = mean(user_word_count, na.rm = TRUE),
    mean_turn_count = mean(user_turn_count, na.rm = TRUE),
    .groups = "drop"
  )

engagement_comparison %>%
  gt() %>%
  cols_label(
    success = "Outcome",
    mean_word_count = "Mean Word Count",
    mean_turn_count = "Mean Turn Count"
  ) %>%
  fmt_number(columns = c(mean_word_count, mean_turn_count), decimals = 1)
```

The qualitative analysis of conversations (full transcripts available in data/processed/conversations/) reveals distinct patterns:

**Successful learners**:

- Approached conversations with genuine curiosity
- Explored substantive policy topics in depth
- Found common ground on shared concerns
- Asked respectful, probing questions
- Created simple, bridge-building marketing messages

**Unsuccessful learners**:

- Remained skeptical or hostile throughout
- Brought external "evidence" to confirm preconceptions
- Challenged the chatbot's authenticity
- Stayed in adversarial mode
- Created divisive or sarcastic marketing messages

These patterns suggest that genuine engagement is crucial for learning across partisan lines.
Participants who used the conversation to confirm existing biases actually became less accurate, while those who approached it with openness showed dramatic improvements.

### Full Conversation Transcripts

The following sections present the complete conversations for each extreme case, illustrating the qualitative differences between successful and unsuccessful learning.

#### D→R Most Improved: R_3smKFSFIo8Nlgir

**Accuracy improvement**: 2.843 (from 2.921 to 0.079)
**Marketing message**: "Energy-saving Appliances, Helps Make America Great!"

```{r conversation-dr-most, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_3smKFSFIo8Nlgir.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This conversation shows deep engagement on economic issues. The participant found genuine common ground on concerns about taxes, regulations, and job stability. The chatbot explored practical implications of reduced government intervention, and the participant asked probing questions about implementation. This substantive exchange led to dramatically improved accuracy and a patriotic marketing message that bridges partisan values.

#### D→R Least Improved: R_7roKHi99S0lQCJq

**Accuracy change**: -1.333 (worsened from 1.088 to 2.421)
**Marketing message**: "made for independence 'lower your costs, stay in control'."

```{r conversation-dr-least, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_7roKHi99S0lQCJq.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This participant appeared distracted (mentioning grading quizzes) and the conversation remained superficial. While practical topics were discussed (appliances, thermostats), the exchange never went deep enough to challenge preconceptions. The participant's attention was divided, and they didn't engage with substantive policy questions. The marketing message reflects this confused engagement.

#### R→D Most Improved: R_7rne5ptEvs5r7Il

**Accuracy improvement**: 3.000 (from 3.251 to 0.251)
**Marketing message**: "energy saving appliances . save energy and our planet"

```{r conversation-rd-most, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_7rne5ptEvs5r7Il.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This wide-ranging conversation explored Democratic values comprehensively—environment, healthcare, social justice, immigration. The participant asked about core beliefs and policy positions, maintaining a respectful tone even when disagreeing. The chatbot provided detailed explanations of Democratic perspectives. This genuine exploration led to dramatic accuracy improvement and an authentic environmental message.

#### R→D Least Improved: R_39bB9XiMhyFaHnj

**Accuracy change**: -2.167 (worsened from 0.751 to 2.918)
**Marketing message**: "Come spend 50% more on less practical appliances since that's what you claim to do and support the business. But I know you won't because just like every Republican said its not financially plausible and your actions only prove the Republicans point."

```{r conversation-rd-least, results='asis'}
convo <- fromJSON("../data/processed/conversations/R_39bB9XiMhyFaHnj.json")

cat("\n")
for (i in 1:nrow(convo)) {
  msg <- convo[i, ]
  role_display <- ifelse(msg$role == "user", "**PARTICIPANT**", "**CHATBOT**")
  cat(sprintf("%s\n\n", role_display))
  cat(sprintf("%s\n\n", msg$content))
  if (i < nrow(convo)) cat("---\n\n")
}
```

**Analysis**: This conversation was hostile from the start. The participant challenged the chatbot's authenticity, cited external "evidence" (a BBC study) to prove Democrats are hypocrites, and remained adversarial throughout. Rather than exploring Democratic perspectives, the participant used the conversation to confirm negative stereotypes. The chatbot tried to acknowledge concerns and find common ground, but the participant rejected these attempts. This confirmation bias approach led to worsened accuracy and an openly hostile marketing message.

---

### Data Availability

Raw data: [data/raw/qualtrics.parquet](../data/raw/qualtrics.parquet)

Cleaned data: [data/processed/cleaned_data.parquet](../data/processed/cleaned_data.parquet)

Analysis data (minimal, for publication): [data/processed/analysis_data.parquet](../data/processed/analysis_data.parquet)

Conversations: [data/processed/conversations/](../data/processed/conversations/)

### Code Availability

Data cleaning: [src/r/clean_data.R](../src/r/clean_data.R)

Analysis dataset creation: [src/r/create_analysis_data.R](../src/r/create_analysis_data.R)

Conversation download: [src/python/download_conversations.py](../src/python/download_conversations.py)

Extreme cases analysis: [src/r/analyze_extreme_cases.R](../src/r/analyze_extreme_cases.R)

### Pre-Registration

This study was pre-registered prior to data collection.
See [docs/pre-registration.md](../docs/pre-registration.md) for complete details including hypotheses, measures, sample size justification, and planned analyses.

### Session Information

```{r session-info}
sessionInfo()
```
